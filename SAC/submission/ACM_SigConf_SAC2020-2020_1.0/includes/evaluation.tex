\section{Evaluation}\label{sec:evaluation}

%Describe the experimental setup, the used datasets/parameters and the experimental results achieved

This section gives an overview of the used datasets, the used metric to evaluate our models, the configuration of our model, and finally an explanation about input shuffling

\subsection{Dataset}\label{sec:dataset}
In this implementation we only used the text8 \footnote{http://mattmahoney.net/dc/enwik8.zip} dataset. We chose this dataset for two reasons. First of all, it's a small dataset, that allowed us to do a lot of computations. Secondly, this dataset was used in related work \citep{intel} hence giving us a very good benchmark. The text8 dataset consists of 1702 lines of 1000 words, with a vocabulary of roughly 63000 words. Conveniently, there is no punctuation in the dataset. Therefore, we had to choose between building arbitrary sentences and keeping the dataset as it is. We chose the first option because it gives us a faster computation time, and did not show any significant loss in quality empirically. We chose a length of sentences of 20. Furthermore, we applied a technique called subsampling to reduce the data set size, which is explained in the following subsection.
\iffalse
\begin{table}[tb]
    \caption{Training and Convergence time according to choice of the length of sentences in text8 datasetCaption}
    %\scriptsize
    \begin{tabular}{l r r r r}%
        \toprule
          Length of Sentences & 10 & 20 & 40 & 1 Document \\ 

        \midrule%
Training Time for one batch &8min & 10min & 11min & 18min \\ 
Convergence Time in epochs &4  & 3  & 3  & 3  \\ 
Word Similarity& 0.65 & 0.66 & 0.66 & 0.66 \\ \hline
        \midrule%
   \end{tabular}%
   \label{table:with_20}%
\end{table}
\fi 

\subsubsection{Subsampling}
Subsampling is a technique introduced by Mikolov et al. \citep{mikolov} to reduce the size of the dataset while at the same increasing the quality of the word embeddings (WE). The idea behind subsampling is the removal of very frequent words such as: "the, as, it, of". These words do not give an intrinsic value to the words that appear in its context. Therefore, the goal of subsampling is to delete such words from the dataset. Subsampling will decrease the computation time, as it will reduce the number of training samples, and should, in theory, increase the accuracy of the model. The increase in accuracy can also be explained by the fact that words that would not have appeared in the context of each other, may now do because words between have been deleted. 
Details about the probability of the deletion of a word can be found in the orignial paper from Mikolov \citep{mikolov2}.
\iffalse
To choose which word to delete, Mikolov et al. \citep{mikolov2} chose the following equation to compute the probability of a word $w$  to be deleted from the data set:
\begin{equation} \label{eq:sampling}
P(w) = 1- \sqrt{{\frac{t}{f(w)}}}
\end{equation}

where $f(w)$ is the frequency of w, and $t$ is a threshold set empirically. Mikolov et al. recommend a value between $0$ and $10^{-5}$, depending on the size of the dataset. We experimented with different values and $10^{-4}$ seemed the most suited on the text8 dataset. We did this by looking at a random set of sentences and judging the results manually. 
As Equation \ref{eq:sampling} is a probability, subsampling is not a deterministic procedure, words that may have been deleted with a threshold of $10^-2$ may stay in the dataset with a lower threshold. 
An example of the first sentence with different sampling thresholds can be found in Table \ref{table:treshold_examples}. The table shows the first 20 words of our dataset, without the words that were subsampled according to a threshold sample. Stats about subsampling can be found in Table \ref{table:threshold}.
\begin{table}[tb]
\caption{Size of the preprocessed text8 dataset according to sampling treshold}
    %\scriptsize
    \begin{tabular}{l r r r r r}%
        \toprule
Sampling Treshhold & 0 & $ 10^{-1}$&$ 10^{-2}$& $10^{-3} $ &$10^{-4} $ \\ 
        \midrule%
Words in Dataset  & 16mio & 15mio & 11mio & 8mio & 4mio \\ 
        \bottomrule%
   \end{tabular}%
   \label{table:threshold}%
\end{table}

\begin{table*}[tb]\centering
\caption{Example of a sentence with different sampling tresholds}
    %\scriptsize
    \begin{tabular}{l l}%
        \toprule
Sampling Treshold & First sentence of Dataset \\ 
        \midrule%
$10^{-1}$ & \makecell[l]{Anarchism originated as a term of abuse first used against early working class radicals including the diggers\\ of the english} \\ \hline
$10^{-2}$  & \makecell[l]{ Anarchism originated as a term of abuse first used against early \\ working class radicals including diggers of english} \\ \hline
$10^{-3}$ & \makecell[l]{Anarchism originated a term abuse first used against early \\ working class radicals including diggers the english}\\ \hline
$10^{-4}$ &\makecell[l]{ Anarchism originated abuse used against working class radicals\\ diggers english} \\ \hline
$10^{-5}$ & against radicals diggers \\ \hline        
        \midrule%
   \end{tabular}%
\label{table:treshold_examples}
\end{table*}
\fi


\subsubsection{Min count}
We also deleted every word that did not appear more than 5 times in our dataset. This idea was also introduced by Mikolove et al. \citep{mikolov2}. This is a good technique because of three reasons: First certain words of our data sets do not appear in a common lexicon (twigleg, melqu), or come from a foreign language (Volksvereinigung), or are names and acronyms. Secondly, each document often has spelling mistakes, those would be deleted by sampling too, as the words do not have any meaning. Lastly, a word that only appears one time in our dataset will be very dependent on its original initialization. For all the above reasons, we applied this technique. Similar to subsampling, it will decrease computation time and  should in theory improve the quality of the word embeddings.

\subsection{Evaluating word embedings}
Evaluating word embedding is not an easy task, such as evaluating the accuracy of a common classifier for example. We cannot split our data set into train and test set. As the task that the network is learning is of no interest to us. Therefore, to ensure the quality of word embeddings other techniques have to be used. To define quality we first need to define a measure of similarity between two vectors. For this task we will use the cosine distance. Two vectors will be judged similiar if they point in the same direction. 


\subsubsection{Word similarity and wordsim353}
To measure the quality of our word embedding, in question of word similarity we must use a dataset to compare our self too.
 We chose wordsim353\footnote{http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip} for this task, as it's the most used in the related literature. The data set consists of 353 pairs of words rated by humans on their similarity. The similarity score is in the range of 1 and 10, with 10 being the best score. An example for two of such pairs can be found in Table \ref{fig:ws353_ex}. We will rank our embeddings on the Pearson correlation coefficient between the cosine distance and the scores attributed by humans, as this is the common procedure. This concludes our description of the evaluation methods of word embeddings. The next section will present the configuration of our network used to optimize the SGNS.
\begin{table}[tb]\centering
\caption{Example of pairs and their rating in wordsim353}
\begin{tabular}{l l l }
        \toprule
Word1 & Word2 & Score \\ \hline
        \midrule%

\textquote{FBI} & \textquote{Investigation} & 8.31 \\ \hline
\textquote{Mars} & \textquote{scientist} & 5.63 \\ \hline
        \midrule%
\end{tabular}
\label{fig:ws353_ex}
\end{table}


\subsection{Configuration of the network}\label{ssec:config}
The SGNS has a lot of possible parameters, that can be tuned. We experimented with different models and finally decided for one that we tried to optimize. The network was configured the following way: 
\begin{itemize}
\item \texttt{Negative Samples:} - \textit{10}
\item \texttt{Context Window:}  - \textit{5}
\item\texttt{Dimension of the embedding}: - \textit{100}
\item \texttt{Batch size}: - \textit{2000}
\item \texttt{Alpha}:  - \textit{(1e-5,1)}
\end{itemize}
This concludes our overview of the possible parameters of our SGNS, in the next subsection the reader will be given a description about the process of input shuffling.

\iffalse
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
Embedding Size & Word Similarity on Gensim \\ \hline
50 & 0.65 \\ \hline
100 & 0.67 \\ \hline
200 & 0.65 \\ \hline
300 & 0.63 \\ \hline
\end{tabular}
\caption{Word similarity in relation to the size of the embedding}
\label{table:gensim_emb_size}
\end{table}
\fi
\subsection{Input Shuffling}\label{ssec:shuffling}
We used input shuffling as a technique to optimize the SGNS.  We will first describe input shuffling in a general way and then explain why we supposed that input shuffling could work well on the SGNS. \\

Let $X = {x_1...x_n}$ be our input data set. Input Shuffling describes the process of taking a random permutation of the dataset as an input at each epoch. The idea behind this technique is to present our optimizer with different  surfaces of our loss function, in order to find the best optimum. Furthermore, it's easier for the neural network to escape a local minimum. As for example if a network, without using shuffling,  had converged to a local minimum after one epoch it probably cannot escape it. But if we change the shape of the loss function, by input shuffling, then there would be a greater probability for the network to escape the local minimum.
\\
There are two reasons why we think that input shuffling is particularly well suited for the SGNS. Firstly, we read our words sequentially, therefore,  words that only appear very early in the dataset will not benefit from the context words being already updated from others. The second idea is that we used the special batch technique described in Section \ref{sec:contribution}. When we do not use input shuffling the same word will appear with all of its context words as training samples in a batch, therefore the updae will be an average of all of these training samples. But if we would use input shuffling instead, then the probability of a word appearing multiple time in one batch would be smaller. In consequence the accuracy of the model should be optimized, hence leading to a better convergence time. 


To summarize the evaluation, we deleted words that appear very often (subsampling) and very infrequently (min count) from our dataset, i.e text8 dataset. To evaluate the resulting word embeddings we defined the cosine distance, which is 1 minus the cosine of the angle of two vectors. We will measure the quality of the embeddings by analyzing the correlation of the cosine distance of pairs of vectors to humanly judged similarity score of the corresponding words, and used the wordsim353 to collect these scores. The reader was also given an explanation about the configuration of our network and introduced to the concept of input shuffling. With all this knowledge we can know present the achieved experimental results in the next section.
