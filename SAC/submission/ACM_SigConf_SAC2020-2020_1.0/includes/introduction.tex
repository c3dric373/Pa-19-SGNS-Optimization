\section{Introduction}\label{sec:introduction}

Representing words as vectors, i.e word embeddings (WE) is a fundamental aspect of Natural Language Processing (NLP). There are two ways to create such WE, either arbitrarily or with the purpose of capturing the semantic  of the words, i.e. vector representations of words that are syntactically or semantically similar will be close to each other in the vector space. By capturing semantic or syntactic meaning WE have shown to facilitate a lot of subsequent NLP tasks, such as entity recognition, machine translation or sentence classification. 
%TODO CITE WORK 
The first attempt to create WE with neural networks was made by Bengio et al. \citep{bengio} but more recently Mikolov et al. \citep{mikolov} introduced a software package called w2vec, which uses a simpler network that produces state of the art results. One of the proposed algorithms in this software package is the Skip-Gram Model (SGM). The SGM is an algorithm, that trains a neural network, on the task of predicting the neighboring words in a sentence. The weights of this network are then used as WE. Mikolov et al. \citep{mikolov2} then introduced an alteration to the SGM called negative sampling. The goal of the Skip-Gram Model with negative sampling (SGNS) is to distinguish for a given word $w$, context words $c_i$ (words that appear close to $w$ in a sentence) from words drawn randomly (i.e negative samples) $k_i$. This is achieved by maximizing the dot product of $w$ and $c_i$ and minimizing the dot product of $w$ and $k_i$.\\
The SGM, espacially the SGNS, gained a lot of attention, as it achieved very good results for a very simplistic model. As a consequence, a lot of effort went into optimizing it. Most of this effort was trying to improve the throughput of the model, i.e the number of words that are processed per second. As remarkable progress was made \citep{intel} , \citep{gpu}.  Both of these approaches and most of the literature are focused on improving the throughput of the model, but not the convergence time.  Therefore one could ask if the convergence time of the SGM can be optimized while at the same time maintaining its accuracy.\\
Our work focused on improving the convergence time of the SGNS. To facilitate our calculations we propose a slightly altered version of the original SNGS, in which we compute the loss for a very large amount of training samples, i.e 2000. The loss of such a batch was computed by taking the sum over the loss of all individual training samples in the specific batch. We combined this batched approach with advanced optimizers and input shuffling to decrease the convergence time of the SGNS. Furthermore we analyzed the distribution of word frequencies of different datasets to create batches of better quality. As a matter of fact a word that appear multiple times in a batch will hinder the quality of the batch. Therefore we focused our work on creating batches that only contained words once. In combination advanced optimizers, input shuffling and our batched approach allowed us to decrease the convergence time of the SGNS. If combined with an optimized throughput these results could lead to an overall decrease in runtime.\\
 The rest of the paper is structured as follows: section \ref{sec:background}  describes the SGM and the SGNS followed by its optimzations in Section \ref{sec:related_work}.  The reader is introduced to our Implementation in Section \ref{sec:contribution}. Furthermore  we will describe the used datasets, the measure applied to compare the quality of word embeddings and input shuffling in Section \ref{sec:evaluation}. Results are presented in  Section \ref{sec:results}. The last part will focus on the discussion of our results, and possible future work in Section \ref{sec:discussion} followed by a conclusion in Section \ref{sec:conclusion}.