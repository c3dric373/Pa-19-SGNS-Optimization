
\section{Conclusion}\label{sec:conclusion}

This work provides an overview of the Skip Gram Model with negative Sampling (SGNS) and the numerous successful attempts of optimizing the throughput of the SGNS. This work also proposes a slighlty altered version of the SGNS, where the idea is to compute the loss over the sum of a high number of training samples, i.e 2000,  instead of computing it for each individually. The batched version allowed us to compute more models and analyze the convergence time faster. We showed that the convergence time and the quality of the created word embeddings are not hindered by our implementation. We used the text8 dataset to train our model and  word similarity as a quality measure for the word embeddings (WE).
To compare our work, Gensim was used, which holds a state of the art implementation of the SGNS. We did achieve the same convergence time as Gensim with Adam as an optimizer and the use of input shuffling. Gensim convereged in 4 epochs to a word similarity of 0.68 and our model  took 4 epochs to achieve a score of 0.68. Those results still need to be confirmed with more datasets. Finally, if this work would be combined with an optimized throughput it  could improve the state of the art run time of the SGNS.