{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "\n",
    "\"Transforms a list of words to a list of sentences with length=len_sen\"\n",
    "def words_to_sentences(words,len_sen=20):\n",
    "    new_ds = []\n",
    "    for i in range(0, len(words), len_sen):\n",
    "        y = [words[i:i + len_sen]]\n",
    "        new_ds.extend(y)\n",
    "    return new_ds\n",
    "\n",
    "# Get dataset online\n",
    "dataset = api.load('text8')\n",
    "\n",
    "# Convert to list of words\n",
    "text8_ds = []\n",
    "for x in dataset: \n",
    "    for y in x:\n",
    "        text8_ds.append(y)\n",
    "\n",
    "# New dataset with sentences of length=20\n",
    "text8_dataset = words_to_sentences(text8_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordsim Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, spatial \n",
    "import csv, numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import spatial \n",
    "#IMPORT DATA\n",
    "def get_wordsim_data():\n",
    "    wordsim_data = [] \n",
    "    with open('./data/wordsim/set1.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for row in reader: \n",
    "            wordsim_data.append(row[0].split(',')[0:3])\n",
    "    del wordsim_data[0]\n",
    "    with open('./data/wordsim/set2.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for i,row in enumerate(reader):\n",
    "            if i!=0:\n",
    "                wordsim_data.append(row[0].split(',')[0:3])\n",
    "\n",
    "    return wordsim_data\n",
    "\n",
    "#len(wordsim_vocab.intersection(text8_dataset_first_sentence.vocab))\n",
    "def wordsim_task(dict_emb):\n",
    "    wordsim_data = get_wordsim_data()\n",
    "    scores = []\n",
    "    distances = []\n",
    "    found = 0\n",
    "    missed = 0\n",
    "    for task in wordsim_data: \n",
    "        if (task[0] in dict_emb.keys() ) and (task[1] in dict_emb.keys()):\n",
    "            found += 1\n",
    "            scores.append(float(task[2]))\n",
    "            distances.append(spatial.distance.cosine(dict_emb[task[0]], dict_emb[task[1]]))\n",
    "        else:\n",
    "            missed += 1\n",
    "            \n",
    "    print('found:',found,'missed:',missed)            \n",
    "            \n",
    "    #return stats.zscore(np.array([x[1] for x in out],dtype=float))\n",
    "    return np.corrcoef(scores,distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size_u, emb_size_v,emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size_u, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(emb_size_v, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))#/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "import numbers\n",
    "import itertools\n",
    "import pdb\n",
    "\n",
    "\n",
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, sentences,power=0.75, neg_samples=5, min_count=5, window=5, sample=1e-4, sample_buffer=1000):\n",
    "        self.sentences = sentences\n",
    "        self.neg_samples=neg_samples\n",
    "        self.min_count = min_count\n",
    "        self.window_size = window\n",
    "        self.vocab_id = defaultdict(int)\n",
    "        self.vocab_ctx = defaultdict(int)\n",
    "        self.index2id = dict()\n",
    "        self.id2index = dict()\n",
    "        self.index2ctx = dict()\n",
    "        self.ctx2index = dict()\n",
    "        self.build_vocab(sentences)\n",
    "        self.ctx_weights = self.make_cum_table(power)\n",
    "        self.downsample_probs = np.zeros_like(self.ctx_weights)\n",
    "        if sample > 0:\n",
    "            self.downsample_probs = 1 - np.sqrt(sample/self.ctx_weights).clip(0,1)\n",
    "        self.pairs = []\n",
    "        self.generate_pairs()\n",
    "        #self.generate_pairs_parallel()\n",
    "        self.ctx_weights = torch.FloatTensor(self.ctx_weights)\n",
    "        self.samples = None\n",
    "        self.sample_idx = -1\n",
    "        self.sample_size = 0\n",
    "        self.sample_buffer = sample_buffer\n",
    "        \n",
    "    def dataset_into_chunks(self,dataset,n_chunks=25):\n",
    "        chunks = []\n",
    "        #print(type(n_chunks))\n",
    "        split = int((len(dataset)/(n_chunks-1)))\n",
    "        for i in range(0, len(dataset), split):\n",
    "                    y = [dataset[i:i + split]]\n",
    "                    chunks.extend(y)\n",
    "        return chunks\n",
    "    def chunks_to_ds(chunks): \n",
    "        out = [x for z in l for x in chunks]\n",
    "        return out\n",
    "        \n",
    "    def sliding_window(self, words):\n",
    "        for pos, word in enumerate(words):\n",
    "            # sliding window (randomly reduced to give more weight to closeby words)\n",
    "            reduction = np.random.randint(self.window_size)\n",
    "            start = max(0, pos - self.window_size + reduction)\n",
    "            for pos2, word2 in enumerate(words[start:(pos + self.window_size + 1 - reduction)], start):\n",
    "                if pos2 != pos:\n",
    "                    yield (self.id2index[word],self.id2index[word2])\n",
    "                    \n",
    "    def generate_pairs_inner(self,sent):\n",
    "        pairs = []\n",
    "        words = [w for w in sent if w in self.vocab_id and self.downsample_probs[self.id2index[w]] < random.random()]\n",
    "        for pair in self.sliding_window(words):\n",
    "            pairs.append(pair)\n",
    "        return pairs\n",
    "        \n",
    "    def generate_pairs(self):  \n",
    "        print('generating pairs')\n",
    "        start = time.time()\n",
    "        p = [self.generate_pairs_inner(s) for s in self.sentences]\n",
    "        self.pairs = list(itertools.chain(*p))\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "        print('pairs generated in ',time_since_start)\n",
    "\n",
    "    def generate_pairs_parallel(self): \n",
    "        print('generating pairs parallel')\n",
    "        start = time.time()\n",
    "        chunks = self.dataset_into_chunks(self.sentences)\n",
    "        manager = multiprocessing.Manager()\n",
    "        return_dict = manager.dict()\n",
    "        threads = []\n",
    "        pairs = []\n",
    "        for index,chunk in enumerate(chunks):\n",
    "            thread = multiprocessing.Process(target=self.generate_pairs_single_thread, args=(chunk,index,return_dict))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        \n",
    "        print('start time calculations')\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)    \n",
    "        time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "        print('pairs generated in ',time_since_start)\n",
    "        for i in range(len(chunks)):\n",
    "            pairs.extend(return_dict[i])\n",
    "        #self.pairs = list(itertools.chain(*return_dict.items()))\n",
    "        \n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "        print('pairs generated parallel  in ',time_since_start)\n",
    "        \n",
    "    def generate_pairs_single_thread(self,chunk,index,return_dict):\n",
    "        pairs = []\n",
    "        for sent in chunk:\n",
    "            words = [w for w in sent if w in self.vocab_id and self.downsample_probs[self.id2index[w]] < random.random()]\n",
    "            for pair in self.sliding_window(words):\n",
    "                pairs.append(pair)\n",
    "        #p = [self.generate_pairs_inner(s) for s in chunk]\n",
    "        #out = list(itertools.chain(*p))\n",
    "        print('thread' + str(index) + ' created' + str(len(pairs)) + \" pairs\")\n",
    "        return_dict[index] = pairs\n",
    "      \n",
    "        \n",
    "    def build_vocab(self,sentences):\n",
    "        print('building vocab')\n",
    "        raw_vocab = defaultdict(int)\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                raw_vocab[word] += 1\n",
    "        self.vocab_id = {k:v for k,v in raw_vocab.items() if v >= self.min_count}\n",
    "        self.vocab_ctx = self.vocab_id \n",
    "        del raw_vocab\n",
    "                \n",
    "        # ctx - index\n",
    "        for word in self.vocab_ctx:\n",
    "            self.ctx2index[word] = len(self.ctx2index)\n",
    "        self.index2ctx = dict(zip(self.ctx2index.values(), self.ctx2index.keys()))\n",
    "        # id - index\n",
    "        self.id2index = self.ctx2index\n",
    "        self.index2id = self.index2ctx\n",
    "        print('vocab build')\n",
    "        \n",
    "        \n",
    "    def make_cum_table(self, power):\n",
    "        pow_frequency = np.array([self.vocab_ctx[self.index2ctx[i]] for i in range(len(self.vocab_ctx))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "    \n",
    "    \n",
    "    def sample_neg(self, count):\n",
    "        if self.sample_idx == -1:\n",
    "            self.sample_size = count*self.sample_buffer\n",
    "            self.samples = np.random.choice(list(self.index2ctx.keys()),size=self.sample_size,replace=True,p=self.ctx_weights)\n",
    "            self.sample_idx = 0\n",
    "        while self.sample_idx + count > len(self.samples):\n",
    "            self.samples = np.random.choice(list(self.index2ctx.keys()),size=self.sample_size,replace=True,p=self.ctx_weights)\n",
    "            self.sample_idx = 0\n",
    "        out = self.samples[self.sample_idx:self.sample_idx+count]\n",
    "        self.sample_idx += count\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pos_u = self.pairs[idx][0]\n",
    "        pos_v = self.pairs[idx][1]\n",
    "        samples = self.sample_neg(self.neg_samples)\n",
    "        while pos_v in samples:\n",
    "            samples = self.sample_neg(self.neg_samples)\n",
    "        return (pos_u,pos_v,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab\n",
      "vocab build\n",
      "generating pairs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f08d5db85197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneg_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2VDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext8_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-ed41109da286>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, power, neg_samples, min_count, window, sample, sample_buffer)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m#self.generate_pairs_parallel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ed41109da286>\u001b[0m in \u001b[0;36mgenerate_pairs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generating pairs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_pairs_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ed41109da286>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generating pairs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_pairs_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ed41109da286>\u001b[0m in \u001b[0;36mgenerate_pairs_inner\u001b[0;34m(self, sent)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_id\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msliding_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-ed41109da286>\u001b[0m in \u001b[0;36msliding_window\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# sliding window (randomly reduced to give more weight to closeby words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpos2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "neg_samples = 10\n",
    "w2v_dataset = W2VDataset(text8_dataset,sample_buffer=500000,neg_samples=neg_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "import numbers\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=10, alpha=0.4, iterations=20, batch_size=2000, \n",
    "                 shuffle=True,use_cuda=True,workers=2,momentum=0,nesterov=False,step_size=1,gamma=1):\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        self.ws_list = []\n",
    "        self.loss_list = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab_id),len(self.data.vocab_id), self.dim)\n",
    "        self.model.to(device)\n",
    "        # Choose wanted optimizer\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha, momentum=momentum,nesterov=nesterov)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=alpha)\n",
    "\n",
    "        #self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "        #self.optimizer = torch.optim.Adagrad(self.model.parameters(), lr=alpha)\n",
    "\n",
    "\n",
    "        self.iterations = iterations\n",
    " \n",
    "    def train_with_loader(self,save_embedding=False):\n",
    "        print('starting training')\n",
    "\n",
    "        self.time=0\n",
    "        for epoch in range(self.iterations):\n",
    "            loader = DataLoader(self.data, self.batch_size, self.shuffle, num_workers=self.workers,pin_memory=True)\n",
    "            tenth = int(len(loader)/10)\n",
    "\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            processed_batches = 0 \n",
    "            pairs = 0\n",
    "            cum_loss = 0 \n",
    "            avg_loss =0\n",
    "            best_loss = 10 \n",
    "            \n",
    "            for i,(pos_u,pos_v,neg_v) in enumerate(loader):\n",
    "                if(i%tenth == 0 ):\n",
    "                    end = time.time()\n",
    "                    hours, rem = divmod(end-start, 3600)\n",
    "                    minutes, seconds = divmod(rem, 60)\n",
    "                    time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "                    if(processed_batches!=0):\n",
    "                        avg_loss = cum_loss / processed_batches\n",
    "                    print(\"0%\" + \"=\" *(int(percent/10))+ str(percent) +\"%, \" + time_since_start + \", cum_loss = {}\".format(cum_loss),end=\"\\r\" )\n",
    "                    percent+=10   \n",
    "                    \n",
    "                pos_v = pos_v.view(len(pos_u),-1)\n",
    "                neg_v = neg_v.view(len(pos_u),-1)\n",
    "                pos_u = pos_u.to(device)\n",
    "                pos_v = pos_v.to(device)\n",
    "                neg_v = neg_v.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                cum_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pairs += len(pos_u)\n",
    "                processed_batches += 1\n",
    "                \n",
    "            print(\"\\n{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "            avg_loss = cum_loss / processed_batches\n",
    "            print(\" {0:d} {1:d} batches, pairs {2:d}, cum loss: {3:.5f}\".format(i,processed_batches, pairs,cum_loss))\n",
    "            self.loss_list.append(cum_loss)\n",
    "            self.time = time_since_start\n",
    "            self.model = self.model.to(cpu)\n",
    "            score = -1*(wordsim_task(self.get_embedding())[0][1])\n",
    "            print(\"Current score on wordsim Task: {}\".format(score))\n",
    "            self.ws_list.append(score)\n",
    "            self.model = self.model.to(gpu)\n",
    "            self.data.generate_pairs()\n",
    "        \n",
    "        if(save_embedding):\n",
    "            self.save_embedding()\n",
    "            \n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.index2id)):\n",
    "            embedding_dict[self.data.index2id[i]]= embedding[i]\n",
    "        return embedding_dict\n",
    "    \n",
    "    def save_embedding(self, with_loss=True):\n",
    "        print('ntm')\n",
    "        # Creating filename\n",
    "        optim = \"Optim\" + str(self.optimizer).split(\" \")[0] + \"_\"\n",
    "        filename = \"dict_emb_\" +  optim + \"_\".join([x + str(y) for x,y in vars(self).items() if isinstance(y, numbers.Number)]) + \".pkl\"\n",
    "        \n",
    "        # Getting Embedding\n",
    "        self.model.to(torch.device('cpu'))\n",
    "        dict_emb = w2v.get_embedding()\n",
    "        \n",
    "        # Adding loss history to embedding\n",
    "        dict_emb['loss_list'] = [x.to(torch.device('cpu')) for x in self.loss_list]\n",
    "        \n",
    "        # Adding score list to embedding \n",
    "        dict_emb['ws_list'] = self.ws_list\n",
    "        \n",
    "                \n",
    "        # Saving time spent to calculate 1 epoch\n",
    "        dict_emb['time'] = self.time\n",
    "        \n",
    "        # Logging\n",
    "        print(\"Saving embedding: {} to disk with ws_score: {} \".format(filename,dict_emb['ws_list']))\n",
    "    \n",
    "        # Writing embedding dictionnary to disk\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(dict_emb, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.loss_list = [x.to(device) for x in self.loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:55:26: Main    : before creating thread\n",
      "09:55:26: Main    : before running thread\n",
      "09:55:26: Thread 1: starting\n",
      "09:55:26: Main    : wait for the thread to finish\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:55:29: Thread 1: finishing\n",
      "09:55:29: Main    : all done\n"
     ]
    }
   ],
   "source": [
    "# Test for Threading \n",
    "import logging\n",
    "\n",
    "import threading\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def thread_function(name):\n",
    "\n",
    "    logging.info(\"Thread %s: starting\", name)\n",
    "\n",
    "    time.sleep(3)\n",
    "    \n",
    "    logging.info(\"Thread %s: finishing\", name)\n",
    "\n",
    "\n",
    "\n",
    "format = \"%(asctime)s: %(message)s\"\n",
    "logging.basicConfig(format=format, level=logging.INFO,datefmt=\"%H:%M:%S\")\n",
    "logging.info(\"Main    : before creating thread\")\n",
    "x = threading.Thread(target=thread_function, args=(1,))\n",
    "print('this is a test')\n",
    "logging.info(\"Main    : before running thread\")\n",
    "x.start()\n",
    "logging.info(\"Main    : wait for the thread to finish\")\n",
    "x.join()\n",
    "logging.info(\"Main    : all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<unknown>, line 37)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/conda/envs/skg/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3267\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-23-310592fbdf6d>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', 'def dataset_into_chunks(dataset,n_chunks=4):\\n    chunks = []\\n    split = int((len(dataset)/(n_chunks-1)))\\n    for i in range(0, len(dataset), split):\\n                y = [dataset[i:i + split]]\\n                chunks.extend(y)\\n    return chunks\\n\\ndef sliding_window(self, words):\\n        for pos, word in enumerate(words):\\n            # sliding window (randomly reduced to give more weight to closeby words)\\n            reduction = np.random.randint(self.window_size)\\n            start = max(0, pos - self.window_size + reduction)\\n            for pos2, word2 in enumerate(words[start:(pos + self.window_size + 1 - reduction)], start):\\n                if pos2 != pos:\\n                    yield (self.id2index[word],self.id2index[word2])\\n                    \\ndef generate_pairs_inner(self,sent):\\n        pairs = []\\n        words = [w for w in sent if w in self.vocab_id and self.downsample_probs[self.id2index[w]] < random.random()]\\n        for pair in self.sliding_window(words):\\n            pairs.append(pair)\\n        return pairs\\n        \\ndef generate_pairs(self):  \\n        print(\\'generating pairs\\')\\n        start = time.time()\\n        p = [self.generate_pairs_inner(s) for s in self.sentences]\\n        self.pairs = list(itertools.chain(*p))\\n        end = time.time()\\n        hours, rem = divmod(end-start, 3600)\\n        minutes, seconds = divmod(rem, 60)\\n        time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\\n        print(\\'pairs generated in \\',time_since_start)\\n\\ndef create_pairs_parrallel(chunks): \\n    \\n')\n",
      "  File \u001b[1;32m\"/opt/conda/envs/skg/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2323\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(magic_arg_s, cell)\n",
      "  File \u001b[1;32m\"<decorator-gen-62>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/opt/conda/envs/skg/lib/python3.7/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/opt/conda/envs/skg/lib/python3.7/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1235\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/opt/conda/envs/skg/lib/python3.7/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m100\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "hello world!\n",
      "foo foo foo foo foo foo foo foo foo foo\n"
     ]
    }
   ],
   "source": [
    "def foo(bar, result, index):\n",
    "    print('hello {0}'.format(bar))\n",
    "    result[index] = \"foo\"\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "threads = [None] * 10\n",
    "results = [None] * 10\n",
    "\n",
    "for i in range(len(threads)):\n",
    "    threads[i] = Thread(target=foo, args=('world!', results, i))\n",
    "    threads[i].start()\n",
    "\n",
    "# do some other stuff\n",
    "\n",
    "for i in range(len(threads)):\n",
    "    threads[i].join()\n",
    "\n",
    "print(\" \".join(results) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunks_to_ds(chunks): \n",
    "    out = [x for z in l for x in chunks]\n",
    "    return out\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W2VDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8f743a375bc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mneg_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW2VDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext8_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_buffer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneg_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'W2VDataset' is not defined"
     ]
    }
   ],
   "source": [
    "neg_samples = 10\n",
    "w2v_dataset = W2VDataset(text8_dataset,sample_buffer=500000,neg_samples=neg_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "0%==========100%, Time:  00:06:19.73, cum_loss = 129997976.0\n",
      "1 epoch of 5\n",
      " 20935 20936 batches, pairs 41871861, cum loss: 130032400.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.5833009804350227\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:01:04.86\n",
      "0%=========90%, Time:  00:06:08.79, cum_loss = 107143000.0\n",
      "2 epoch of 5\n",
      " 20939 20940 batches, pairs 41879277, cum loss: 118943048.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6553239607602626\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:01:12.53\n",
      "0%=========90%, Time:  00:09:37.82, cum_loss = 105035128.0\n",
      "3 epoch of 5\n",
      " 20939 20940 batches, pairs 41879278, cum loss: 116693992.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6607751923223647\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:01:14.65\n",
      "0%=========90%, Time:  00:07:21.88, cum_loss = 104084416.0\n",
      "4 epoch of 5\n",
      " 20949 20950 batches, pairs 41899287, cum loss: 115669240.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6576996032875272\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:01:13.24\n",
      "0%==========100%, Time:  00:06:40.41, cum_loss = 114993112.0\n",
      "5 epoch of 5\n",
      " 20947 20948 batches, pairs 41895540, cum loss: 115035840.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6523945134992197\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:01:09.56\n",
      "starting training\n",
      "0%========80%, Time:  00:07:43.57, cum_loss = 105650168.0\r"
     ]
    }
   ],
   "source": [
    "ws_lists = []\n",
    "for x in range(10):\n",
    "    w2v = W2V(w2v_dataset, neg_samples=neg_samples, alpha=0.001,shuffle=True,workers=4,iterations=5,batch_size=2000)\n",
    "    w2v.train_with_loader()\n",
    "    ws_lists.append(w2v.ws_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
