{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text8 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "\n",
    "\"Transforms a list of words to a list of sentences with length=len_sen\"\n",
    "def words_to_sentences(words,len_sen=20):\n",
    "    new_ds = []\n",
    "    for i in range(0, len(words), len_sen):\n",
    "        y = [words[i:i + len_sen]]\n",
    "        new_ds.extend(y)\n",
    "    return new_ds\n",
    "\n",
    "# Get dataset online\n",
    "dataset = api.load('text8')\n",
    "\n",
    "# Convert to list of words\n",
    "text8_ds = []\n",
    "for x in dataset: \n",
    "    for y in x:\n",
    "        text8_ds.append(y)\n",
    "\n",
    "# New dataset with sentences of length=20\n",
    "text8_dataset = words_to_sentences(text8_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordsim Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, spatial \n",
    "import csv, numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import spatial \n",
    "#IMPORT DATA\n",
    "def get_wordsim_data():\n",
    "    wordsim_data = [] \n",
    "    with open('./data/wordsim/set1.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for row in reader: \n",
    "            wordsim_data.append(row[0].split(',')[0:3])\n",
    "    del wordsim_data[0]\n",
    "    with open('./data/wordsim/set2.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for i,row in enumerate(reader):\n",
    "            if i!=0:\n",
    "                wordsim_data.append(row[0].split(',')[0:3])\n",
    "\n",
    "    return wordsim_data\n",
    "\n",
    "#len(wordsim_vocab.intersection(text8_dataset_first_sentence.vocab))\n",
    "def wordsim_task(dict_emb):\n",
    "    wordsim_data = get_wordsim_data()\n",
    "    scores = []\n",
    "    distances = []\n",
    "    found = 0\n",
    "    missed = 0\n",
    "    for task in wordsim_data: \n",
    "        if (task[0] in dict_emb.keys() ) and (task[1] in dict_emb.keys()):\n",
    "            found += 1\n",
    "            scores.append(float(task[2]))\n",
    "            distances.append(spatial.distance.cosine(dict_emb[task[0]], dict_emb[task[1]]))\n",
    "        else:\n",
    "            missed += 1\n",
    "            \n",
    "    print('found:',found,'missed:',missed)            \n",
    "            \n",
    "    #return stats.zscore(np.array([x[1] for x in out],dtype=float))\n",
    "    return np.corrcoef(scores,distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_size_u, emb_size_v,emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(emb_size_u, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(emb_size_v, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0, 0)\n",
    "        \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        neg_v = neg_v.view(len(pos_u),-1)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))#/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import random\n",
    "import time\n",
    "import numbers\n",
    "import itertools\n",
    "\n",
    "class W2VDataset(Dataset):\n",
    "    def __init__(self, sentences,power=0.75, neg_samples=5, min_count=5, window=5, sample=1e-4, sample_buffer=1000):\n",
    "        self.sentences = sentences\n",
    "        self.neg_samples=neg_samples\n",
    "        self.min_count = min_count\n",
    "        self.window_size = window\n",
    "        self.vocab_id = defaultdict(int)\n",
    "        self.vocab_ctx = defaultdict(int)\n",
    "        self.index2id = dict()\n",
    "        self.id2index = dict()\n",
    "        self.index2ctx = dict()\n",
    "        self.ctx2index = dict()\n",
    "        self.build_vocab(sentences)\n",
    "        self.ctx_weights = self.make_cum_table(power)\n",
    "        self.downsample_probs = np.zeros_like(self.ctx_weights)\n",
    "        if sample > 0:\n",
    "            self.downsample_probs = 1 - np.sqrt(sample/self.ctx_weights).clip(0,1)\n",
    "        self.pairs = []\n",
    "        self.generate_pairs()\n",
    "        self.ctx_weights = torch.FloatTensor(self.ctx_weights)\n",
    "        self.samples = None\n",
    "        self.sample_idx = -1\n",
    "        self.sample_size = 0\n",
    "        self.sample_buffer = sample_buffer\n",
    "        \n",
    "    def sliding_window(self, words):\n",
    "        for pos, word in enumerate(words):\n",
    "            # sliding window (randomly reduced to give more weight to closeby words)\n",
    "            reduction = np.random.randint(self.window_size)\n",
    "            start = max(0, pos - self.window_size + reduction)\n",
    "            for pos2, word2 in enumerate(words[start:(pos + self.window_size + 1 - reduction)], start):\n",
    "                if pos2 != pos:\n",
    "                    yield (self.id2index[word],self.id2index[word2])\n",
    "                    \n",
    "    def generate_pairs_inner(self,sent):\n",
    "        pairs = []\n",
    "        words = [w for w in sent if w in self.vocab_id and self.downsample_probs[self.id2index[w]] < random.random()]\n",
    "        for pair in self.sliding_window(words):\n",
    "            pairs.append(pair)\n",
    "        return pairs\n",
    "        \n",
    "    def generate_pairs(self):  \n",
    "        print('generating pairs')\n",
    "        start = time.time()\n",
    "        p = [self.generate_pairs_inner(s) for s in self.sentences]\n",
    "        self.pairs = list(itertools.chain(*p))\n",
    "        end = time.time()\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "        print('pairs generated in ',time_since_start)\n",
    "        \n",
    "        \n",
    "    def build_vocab(self,sentences):\n",
    "        print('building vocab')\n",
    "        raw_vocab = defaultdict(int)\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                raw_vocab[word] += 1\n",
    "        self.vocab_id = {k:v for k,v in raw_vocab.items() if v >= self.min_count}\n",
    "        self.vocab_ctx = self.vocab_id \n",
    "        del raw_vocab\n",
    "                \n",
    "        # ctx - index\n",
    "        for word in self.vocab_ctx:\n",
    "            self.ctx2index[word] = len(self.ctx2index)\n",
    "        self.index2ctx = dict(zip(self.ctx2index.values(), self.ctx2index.keys()))\n",
    "        # id - index\n",
    "        self.id2index = self.ctx2index\n",
    "        self.index2id = self.index2ctx\n",
    "        print('vocab build')\n",
    "        \n",
    "        \n",
    "    def make_cum_table(self, power):\n",
    "        pow_frequency = np.array([self.vocab_ctx[self.index2ctx[i]] for i in range(len(self.vocab_ctx))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "    \n",
    "    \n",
    "    def sample_neg(self, count):\n",
    "        if self.sample_idx == -1:\n",
    "            self.sample_size = count*self.sample_buffer\n",
    "            self.samples = np.random.choice(list(self.index2ctx.keys()),size=self.sample_size,replace=True,p=self.ctx_weights)\n",
    "            self.sample_idx = 0\n",
    "        while self.sample_idx + count > len(self.samples):\n",
    "            self.samples = np.random.choice(list(self.index2ctx.keys()),size=self.sample_size,replace=True,p=self.ctx_weights)\n",
    "            self.sample_idx = 0\n",
    "        out = self.samples[self.sample_idx:self.sample_idx+count]\n",
    "        self.sample_idx += count\n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pos_u = self.pairs[idx][0]\n",
    "        pos_v = self.pairs[idx][1]\n",
    "        samples = self.sample_neg(self.neg_samples)\n",
    "        while pos_v in samples:\n",
    "            samples = self.sample_neg(self.neg_samples)\n",
    "        return (pos_u,pos_v,samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "import numbers\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=10, alpha=0.4, iterations=20, batch_size=2000, \n",
    "                 shuffle=True,use_cuda=True,workers=2,momentum=0,nesterov=False,step_size=1,gamma=1):\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        self.ws_list = []\n",
    "        self.loss_list = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab_id),len(self.data.vocab_id), self.dim)\n",
    "        self.model.to(device)\n",
    "        # Choose wanted optimizer\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha, momentum=momentum,nesterov=nesterov)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=alpha)\n",
    "\n",
    "        #self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "        #self.optimizer = torch.optim.Adagrad(self.model.parameters(), lr=alpha)\n",
    "\n",
    "\n",
    "        self.iterations = iterations\n",
    " \n",
    "    def train_with_loader(self,save_embedding=False):\n",
    "        print('starting training')\n",
    "\n",
    "        self.time=0\n",
    "        for epoch in range(self.iterations):\n",
    "            loader = DataLoader(self.data, self.batch_size, self.shuffle, num_workers=self.workers,pin_memory=True)\n",
    "            tenth = int(len(loader)/10)\n",
    "\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            processed_batches = 0 \n",
    "            pairs = 0\n",
    "            cum_loss = 0 \n",
    "            avg_loss =0\n",
    "            best_loss = 10 \n",
    "            \n",
    "            for i,(pos_u,pos_v,neg_v) in enumerate(loader):\n",
    "                if(i%tenth == 0 ):\n",
    "                    end = time.time()\n",
    "                    hours, rem = divmod(end-start, 3600)\n",
    "                    minutes, seconds = divmod(rem, 60)\n",
    "                    time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "                    if(processed_batches!=0):\n",
    "                        avg_loss = cum_loss / processed_batches\n",
    "                    print(\"0%\" + \"=\" *(int(percent/10))+ str(percent) +\"%, \" + time_since_start + \", cum_loss = {}\".format(cum_loss),end=\"\\r\" )\n",
    "                    percent+=10   \n",
    "                    \n",
    "                pos_v = pos_v.view(len(pos_u),-1)\n",
    "                neg_v = neg_v.view(len(pos_u),-1)\n",
    "                pos_u = pos_u.to(device)\n",
    "                pos_v = pos_v.to(device)\n",
    "                neg_v = neg_v.to(device)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                cum_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pairs += len(pos_u)\n",
    "                processed_batches += 1\n",
    "                \n",
    "            print(\"\\n{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "            avg_loss = cum_loss / processed_batches\n",
    "            print(\" {0:d} {1:d} batches, pairs {2:d}, cum loss: {3:.5f}\".format(i,processed_batches, pairs,cum_loss))\n",
    "            self.loss_list.append(cum_loss)\n",
    "            self.time = time_since_start\n",
    "            self.model = self.model.to(cpu)\n",
    "            score = -1*(wordsim_task(self.get_embedding())[0][1])\n",
    "            print(\"Current score on wordsim Task: {}\".format(score))\n",
    "            self.ws_list.append(score)\n",
    "            self.model = self.model.to(gpu)\n",
    "            self.data.generate_pairs()\n",
    "        \n",
    "        if(save_embedding):\n",
    "            self.save_embedding()\n",
    "            \n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.index2id)):\n",
    "            embedding_dict[self.data.index2id[i]]= embedding[i]\n",
    "        return embedding_dict\n",
    "    \n",
    "    def save_embedding(self, with_loss=True):\n",
    "        print('ntm')\n",
    "        # Creating filename\n",
    "        optim = \"Optim\" + str(self.optimizer).split(\" \")[0] + \"_\"\n",
    "        filename = \"dict_emb_\" +  optim + \"_\".join([x + str(y) for x,y in vars(self).items() if isinstance(y, numbers.Number)]) + \".pkl\"\n",
    "        \n",
    "        # Getting Embedding\n",
    "        self.model.to(torch.device('cpu'))\n",
    "        dict_emb = w2v.get_embedding()\n",
    "        \n",
    "        # Adding loss history to embedding\n",
    "        dict_emb['loss_list'] = [x.to(torch.device('cpu')) for x in self.loss_list]\n",
    "        \n",
    "        # Adding score list to embedding \n",
    "        dict_emb['ws_list'] = self.ws_list\n",
    "        \n",
    "                \n",
    "        # Saving time spent to calculate 1 epoch\n",
    "        dict_emb['time'] = self.time\n",
    "        \n",
    "        # Logging\n",
    "        print(\"Saving embedding: {} to disk with ws_score: {} \".format(filename,dict_emb['ws_list']))\n",
    "    \n",
    "        # Writing embedding dictionnary to disk\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(dict_emb, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.loss_list = [x.to(device) for x in self.loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab\n",
      "vocab build\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:00:29.57\n",
      "starting training\n",
      "0%==========100%, Time:  00:01:57.79, cum_loss = 130110584.0\n",
      "1 epoch of 5\n",
      " 20942 20943 batches, pairs 41884143, cum loss: 130122448.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.610248151375615\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:00:32.63\n",
      "0%=========90%, Time:  00:01:41.81, cum_loss = 107162168.0\n",
      "2 epoch of 5\n",
      " 20939 20940 batches, pairs 41879606, cum loss: 118966512.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6529726034610638\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:00:31.76\n",
      "0%==========100%, Time:  00:01:52.45, cum_loss = 116653648.0\n",
      "3 epoch of 5\n",
      " 20938 20939 batches, pairs 41877566, cum loss: 116702632.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6674588897819405\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:00:32.77\n",
      "0%==========100%, Time:  00:01:54.07, cum_loss = 115567080.0\n",
      "4 epoch of 5\n",
      " 20934 20935 batches, pairs 41868849, cum loss: 115591400.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6673891344168892\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:00:32.29\n",
      "0%==========100%, Time:  00:01:55.70, cum_loss = 114929720.0\n",
      "5 epoch of 5\n",
      " 20938 20939 batches, pairs 41877579, cum loss: 114978256.00000\n",
      "found: 333 missed: 20\n",
      "Current score on wordsim Task: 0.6695212998086839\n",
      "generating pairs\n",
      "pairs generated in  Time:  00:00:31.38\n"
     ]
    }
   ],
   "source": [
    "neg_samples = 10\n",
    "w2v_dataset = W2VDataset(text8_dataset,sample_buffer=500000,neg_samples=neg_samples)\n",
    "w2v = W2V(w2v_dataset, neg_samples=neg_samples, alpha=0.001,shuffle=True,workers=4,iterations=5,batch_size=2000)\n",
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
