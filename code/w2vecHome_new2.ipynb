{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting and preprocessing the data\n",
    "First we get the dataset online, then apply subsampling, then divide the dataset in equally long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.68 s, sys: 91.7 ms, total: 1.77 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import dropwhile\n",
    "\n",
    "def sampling(dataset,threshold=1e-4, min_count=5):\n",
    "    \n",
    "    # Count occurences of each word in the dataset \n",
    "    word_counts = Counter(dataset)  \n",
    "    total_count = len(dataset)\n",
    "    \n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    print(p_drop['the'])\n",
    "    train_words = [word for word in dataset if((random.random() < (1 - p_drop[word])) and (word_counts[word]>min_count))]\n",
    "    #del dataset\n",
    "    return train_words\n",
    "\n",
    "\"Transforms a list of words to a list of sentences with length=len_sen\"\n",
    "def words_to_sentences(words):\n",
    "    new_ds = []\n",
    "    len_sen = int(len(words)/1700)\n",
    "    len_sen = 20\n",
    "    for i in range(0, len(words), len_sen):\n",
    "        y = [words[i:i + len_sen]]\n",
    "        new_ds.extend(y)\n",
    "    return new_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENWIK9 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"../../enwik9\")\n",
    "enwik9 = file.readlines()\n",
    "enwik9 = enwik9[0].split()\n",
    "#enwik9 = sampling(enwik9)\n",
    "#enwik9 = words_to_sentences(enwik9)\n",
    "#with open(\"enwik9_sampled_1e-4_as_list\", 'wb') as output:\n",
    "    #pickle.dump(enwik9, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../../enwik9_sampled_1e-4_as_list\", 'rb') as output:\n",
    "        enwik9 = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(enwik9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwik9_without_min = []\n",
    "for x in enwik9:\n",
    "    if (word_counts[x] >5) and (word_counts[x]< 1500) :\n",
    "        enwik9_without_min.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwik9_without_outliers_sentences = words_to_sentences(enwik9_without_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = Counter(enwik9_without_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "freq_list = list(word_counts.values())\n",
    "print(np.percentile(freq_list,75))\n",
    "np.percentile(freq_list,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 + (64-9)*1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "enwik9_without_outliers= []\n",
    "for x in enwik9_without_min:\n",
    "    if ((word_counts[x])<1500):\n",
    "        enwik9_without_outliers.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(enwik9_without_outliers)& set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=[]\n",
    "s = set(enwik9_without_outliers)\n",
    "for task in wordsim_data: \n",
    "        if (task[0] in s ) and (task[1] in s):\n",
    "            t.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXT8 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntm\n",
      "0.9599730740166865\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import gensim.downloader as api\n",
    "# Get dataset online\n",
    "dataset = api.load('text8')\n",
    "print('ntm')\n",
    "# Convert to list of words\n",
    "text8_ds = []\n",
    "for x in dataset: \n",
    "    for y in x:\n",
    "        text8_ds.append(y)\n",
    "        \n",
    "# Subsampling\n",
    "text8_ds = sampling(text8_ds)\n",
    "\n",
    "# New dataset with sentences of length=20\n",
    "#sampling_dict_text8 = sampling(text8_ds)\n",
    "\n",
    "text8_ds_min_count = []\n",
    "word_counts = Counter(text8_ds)\n",
    "for x in text8_ds: \n",
    "    if ((word_counts[x] > 5) and (word_counts[x]<144 )):\n",
    "        text8_ds_min_count.append(x)\n",
    "        \n",
    "        \n",
    "word_counts = Counter(text8_ds_min_count)\n",
    "len(word_counts)\n",
    "#text8_dataset = words_to_sentences(text8_ds)\n",
    "text8_dataset = words_to_sentences(text8_ds_min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54515"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16680600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16680599"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text8_dataset)*20)\n",
    "len(text8_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats, spatial \n",
    "import csv, numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy import spatial \n",
    "#IMPORT DATA\n",
    "def get_wordsim_data():\n",
    "    wordsim_data = [] \n",
    "    with open('./data/wordsim/set1.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for row in reader: \n",
    "            wordsim_data.append(row[0].split(',')[0:3])\n",
    "    del wordsim_data[0]\n",
    "    with open('./data/wordsim/set2.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ',quotechar='|')\n",
    "        for i,row in enumerate(reader):\n",
    "            if i!=0:\n",
    "                wordsim_data.append(row[0].split(',')[0:3])\n",
    "\n",
    "    wordsim_vocab = set()\n",
    "    for x in wordsim_data:\n",
    "        wordsim_vocab.add(x[0])\n",
    "        wordsim_vocab.add(x[1])\n",
    "    return wordsim_data\n",
    "\n",
    "#len(wordsim_vocab.intersection(text8_dataset_first_sentence.vocab))\n",
    "def wordsim_task(dict_emb):\n",
    "    wordsim_data = get_wordsim_data()\n",
    "    scores = []\n",
    "    distances = []\n",
    "    for task in wordsim_data: \n",
    "        if (task[0] in dict_emb.keys() ) and (task[1] in dict_emb.keys()):\n",
    "            scores.append(float(task[2]))\n",
    "            distances.append(spatial.distance.cosine(dict_emb[task[0]], dict_emb[task[1]]))\n",
    "            \n",
    "            \n",
    "    #return stats.zscore(np.array([x[1] for x in out],dtype=float))\n",
    "    return np.corrcoef(scores,distances)\n",
    "\n",
    "#print(wordsim_task(gensim_emb))\n",
    "#wordsim_task(dict_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dimension):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.emb_dimension = emb_dimension\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, emb_dimension, sparse=False)\n",
    "        self.init_emb()\n",
    "        \n",
    "\n",
    "    def init_emb(self):\n",
    "        initrange = 0.5 / self.emb_dimension\n",
    "        self.u_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.v_embeddings.weight.data.uniform_(-0,0)\n",
    "        \n",
    "            \n",
    "    def forward(self, pos_u, pos_v,neg_v):\n",
    "        pos_u = pos_u.view(-1).to(device)\n",
    "        pos_v = pos_v.to(device)\n",
    "        neg_v = neg_v.to(device)\n",
    "        emb_u = self.u_embeddings(pos_u)\n",
    "        samples = torch.cat([pos_v,Variable(neg_v)],1)\n",
    "        emb_v = self.v_embeddings(samples)\n",
    "        score = torch.bmm(emb_v, emb_u.unsqueeze(2)).squeeze()\n",
    "        score[:,1:]=score[:,1:].neg()\n",
    "        score = F.logsigmoid(score)\n",
    "        return -1 * (torch.sum(score))/ pos_u.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class wDataSet(Dataset):\n",
    "    def __init__(self, dataset, sampling_dict, power=0.75,ctx_window=5):\n",
    "        self.LEN_SEN =20\n",
    "        self.sampling_dict=sampling_dict\n",
    "        #assert( all(len(sentence)== self.LEN_SEN) for sentence in dataset)\n",
    "        self.ctx_window = ctx_window\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = dict()\n",
    "        self.idx2word = dict()\n",
    "        self.word_count = defaultdict(int)\n",
    "        self.vocab_size = int()\n",
    "        self.vocab = set()\n",
    "        self.create_vocab()\n",
    "        self.pairs = self.generate_pairs()\n",
    "        self.key_pairs = self.generate_key_pairs(self.pairs)\n",
    "        self.power = power        \n",
    "        self.neg_table = self.make_neg_table(self.power)\n",
    "        #self.len = self.__len__()\n",
    "        self.build_pair_dict()\n",
    "        \n",
    "    \n",
    "    def build_pair_dict(self):\n",
    "        self.pair_dict = defaultdict(list)\n",
    "        for x,y in self.key_pairs:\n",
    "            self.pair_dict[x].append(y)\n",
    "            \n",
    "\n",
    "        \n",
    "    def generate_pairs(self):\n",
    "        print(\"Generating pairs\")\n",
    "        pairs = []\n",
    "        for sentence in self.dataset:\n",
    "            for i,word in enumerate(sentence):\n",
    "                #if(random.random() < 1- self.sampling_dict[word]):\n",
    "                    for j in range(1,self.ctx_window+1):\n",
    "                        if(i+j<len(sentence)):\n",
    "                            pairs.append((word,sentence[i+j]))\n",
    "                        if((i-j)>=0):\n",
    "                            pairs.append((word,sentence[i-j]))\n",
    "        return pairs\n",
    "        \n",
    "    def __len__(self):          \n",
    "        len_dataset = len(self.dataset)     \n",
    "        center_pairs = ((self.LEN_SEN - self.ctx_window*2)*self.ctx_window*2) \n",
    "        border_pairs = sum([self.ctx_window + i for i in range(self.ctx_window)])*2\n",
    "        len_sen_without_last = (center_pairs + border_pairs)* (len_dataset-1)\n",
    "        \n",
    "        # The last sentence does not has the same length as the other ones, hence it's length needs to be computed otherwise\n",
    "        len_last_sen = len(self.dataset[(len_dataset-1)])\n",
    "        pairs_last_sen = 0\n",
    "        for j in range(len_last_sen):\n",
    "            if(j<self.ctx_window):\n",
    "                # Checking if the rest of the sentence is smaller then the context window\n",
    "                if(j+self.ctx_window >= len_last_sen):\n",
    "                    diff = len_last_sen - 1- j \n",
    "                    pairs_last_sen += diff\n",
    "                    pairs_last_sen += j\n",
    "                else:\n",
    "                    pairs_last_sen += (j+self.ctx_window)\n",
    "            elif( j>= len_last_sen - self.ctx_window):\n",
    "                pairs_last_sen += (len_last_sen-1-j+self.ctx_window)\n",
    "            else:\n",
    "                pairs_last_sen += (2*self.ctx_window)\n",
    "    \n",
    "        return len_sen_without_last + pairs_last_sen\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    def get_neg_samples(self, count, batch_size):\n",
    "        return torch.tensor(np.random.choice(list(self.idx2word.keys()),size=(batch_size)*count,replace=True,p=self.neg_table)).view(batch_size,-1)\n",
    "   \n",
    "    \"\"\" Defines the probability of choosing a negative sampling, set empiraccaly by mikolov\"\"\"\n",
    "    def make_neg_table(self, power):\n",
    "        pow_frequency = np.array([self.word_count[self.idx2word[i]] for i in range(len(self.word_count))])**power\n",
    "        return pow_frequency / pow_frequency.sum()\n",
    "        \n",
    "\n",
    "    def generate_key_pairs(self,pairs):\n",
    "        print(\"Generating key_pairs\")\n",
    "        key_pairs = []\n",
    "        for x,y in pairs:\n",
    "            key_pairs.append((self.word2idx.get(x),self.word2idx.get(y)))\n",
    "        print(\"finished creating key_pairs\")\n",
    "        return key_pairs\n",
    "    \n",
    "    \"\"\"\"Creating vocabulary and creating dictionary with a one to one mapping int to word\"\"\"\n",
    "    def create_vocab(self):\n",
    "        print(\"Creating vocab\")\n",
    "        for i,sentence in enumerate(self.dataset):\n",
    "            for word in sentence:\n",
    "                self.word_count[word] += 1\n",
    "                self.vocab.add(word)\n",
    "        self.word2idx = {w: idx for (idx, w) in enumerate(self.vocab)}\n",
    "        self.idx2word = {idx: w for (idx, w) in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "             \n",
    "    def __getitem__(self, idx):\n",
    "        #Getting the number of pairs per sentence\n",
    "        border_pairs = sum([self.ctx_window + i for i in range(self.ctx_window)])*2\n",
    "        center_pairs = ((self.LEN_SEN - self.ctx_window*2)*self.ctx_window*2)\n",
    "        n_pairs_in_sen = border_pairs + center_pairs\n",
    "        id_sen = int(idx/n_pairs_in_sen)\n",
    "        sen  = self.dataset[id_sen]\n",
    "        pair_id_in_sen = idx - id_sen*(n_pairs_in_sen)\n",
    "        counter = 0 \n",
    "        for i,word in enumerate(sen):\n",
    "            for j in range(1,self.ctx_window+1):\n",
    "                if(i+j< len(sen)):\n",
    "                    if(counter == pair_id_in_sen):\n",
    "                        return(self.word2idx[word],self.word2idx[sen[i+j]])\n",
    "                    counter+=1\n",
    "                    \n",
    "                if(i-j>=0):\n",
    "                    if(counter == pair_id_in_sen):\n",
    "                        return(self.word2idx[word],self.word2idx[sen[i-j]])\n",
    "                    counter+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import time\n",
    "import numbers\n",
    "import pdb\n",
    "\n",
    "class W2V():\n",
    "    def __init__(self, data,dim=100, neg_samples=10, alpha=0.4, iterations=10, batch_size=2000, \n",
    "                 shuffle=True,use_cuda=True,workers=2,momentum=0,nesterov=False,step_size=1,gamma=1):\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "        self.shuffle = shuffle        \n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.dim = dim\n",
    "        self.data = data\n",
    "        self.workers = workers\n",
    "        self.ctxw = self.data.ctx_window\n",
    "        self.neg_samples = neg_samples\n",
    "        self.use_cuda = use_cuda\n",
    "        self.models = []\n",
    "        self.optimizers = []\n",
    "        self.ws_list = []\n",
    "        self.loss_list = []\n",
    "        self.model = SkipGramModel(len(self.data.vocab), self.dim)\n",
    "        self.model.to(device)\n",
    "    \n",
    "        print(device)\n",
    "        #self.optimizer = torch.optim.SGD(self.model.parameters(), lr=alpha, momentum=momentum,nesterov=nesterov)\n",
    "        #self.scheduler = StepLR(self.optimizer, step_size=step_size, gamma=gamma)\n",
    "        #self.optimizer = torch.optim.Adagrad(self.model.parameters(), lr=alpha)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(),lr=alpha)\n",
    "\n",
    "\n",
    "        self.iterations = iterations\n",
    " \n",
    "    def train_with_loader(self,save_embedding=True):\n",
    "        loader = DataLoader(self.data.key_pairs, self.batch_size, self.shuffle, num_workers=self.workers)\n",
    "        print('starting training')\n",
    "       \n",
    "\n",
    "        self.time=0\n",
    "        no_improvement = 0\n",
    "        best_score = -1\n",
    "        prev_score = -1\n",
    "        \n",
    "        \n",
    "        max_key = max(ds.pair_dict, key= lambda x: len(set(ds.pair_dict[x])))\n",
    "        max_value = len(self.data.pair_dict[max_key])\n",
    "        fifth = int(max_value/10)\n",
    "        \n",
    "        for epoch in range(self.iterations):\n",
    "\n",
    "            percent = 0\n",
    "            start = time.time()\n",
    "            processed_batches = 0 \n",
    "            pairs = 0\n",
    "            cum_loss = 0 \n",
    "            avg_loss =0\n",
    "            best_loss = 10 \n",
    "            \n",
    "\n",
    "            for i in range(max_value):\n",
    "                pos_v = []\n",
    "                pos_u = []\n",
    "                for x,y in ds.pair_dict.items():\n",
    "                    if(len(y)>i):\n",
    "                        pos_v.append(y[i])\n",
    "                        pos_u.append(x)\n",
    "                #if(i > 102):\n",
    "                 #       break\n",
    "                pos_v = torch.tensor(pos_v)\n",
    "                pos_u = torch.tensor(pos_u)\n",
    "                end = time.time()\n",
    "                hours, rem = divmod(end-start, 3600)\n",
    "                minutes, seconds = divmod(rem, 60)\n",
    "                time_since_start = \"Time:  {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)\n",
    "                print(\"batch_size = \" + str(len(pos_v)) + \" || processed_batches = \" + str(i) + \"/\" + str(max_value) + time_since_start,end =\"\\r\")\n",
    "                \n",
    "                 \n",
    "               \n",
    "                    \n",
    "                neg_v = self.data.get_neg_samples(self.neg_samples,pos_v.size()[0])\n",
    "                pos_v = pos_v.view(len(neg_v),-1)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self.model.forward(pos_u,pos_v,neg_v)\n",
    "                cum_loss += loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                pairs += len(pos_u)\n",
    "                processed_batches += 1\n",
    "                \n",
    "            print(\"\\n{0:d} epoch of {1:d}\".format(epoch+1, self.iterations))\n",
    "            avg_loss = cum_loss / processed_batches\n",
    "            print(\" {0:d} {1:d} batches, pairs {2:d}, cum loss: {3:.5f}\".format(i,processed_batches, pairs,cum_loss))\n",
    "            self.loss_list.append(cum_loss)\n",
    "            self.time = time_since_start\n",
    "            self.model = self.model.to(cpu)\n",
    "            score = -1*(wordsim_task(self.get_embedding())[0][1])\n",
    "            if(score < best_score):\n",
    "                best_score = score\n",
    "            print(\"Current score on wordsim Task: {}\".format(score))\n",
    "            self.ws_list.append(score)\n",
    "            self.model = self.model.to(gpu)\n",
    "            \n",
    "        \n",
    "            \n",
    "            if(score > 0.68):\n",
    "                print('Model converged')\n",
    "                break\n",
    "        \n",
    "        if(save_embedding):\n",
    "            self.save_embedding()\n",
    "            \n",
    "\n",
    "    def get_embedding(self):\n",
    "        embedding_dict = dict()\n",
    "        embedding = self.model.u_embeddings.weight.data.numpy()\n",
    "        for i in range(len(self.data.idx2word)):\n",
    "            embedding_dict[self.data.idx2word[i]]= embedding[i]\n",
    "        return embedding_dict\n",
    "    \n",
    "    def save_embedding(self, with_loss=True):\n",
    "        print('ntm')\n",
    "        # Creating filename\n",
    "        optim = \"Optim\" + str(self.optimizer).split(\" \")[0] + \"_\"\n",
    "        filename = \"dict_emb_\" +  optim + \"_\".join([x + str(y) for x,y in vars(self).items() if isinstance(y, numbers.Number)]) + \".pkl\"\n",
    "        \n",
    "        # Getting Embedding\n",
    "        self.model.to(torch.device('cpu'))\n",
    "        dict_emb = w2v.get_embedding()\n",
    "        \n",
    "        # Adding loss history to embedding\n",
    "        dict_emb['loss_list'] = [x.to(torch.device('cpu')) for x in self.loss_list]\n",
    "        \n",
    "        # Adding score list to embedding \n",
    "        dict_emb['ws_list'] = self.ws_list\n",
    "        \n",
    "                \n",
    "        # Saving time spent to calculate 1 epoch\n",
    "        dict_emb['time'] = self.time\n",
    "        \n",
    "        # Logging\n",
    "        print(\"Saving embedding: {} to disk with ws_score: {} \".format(filename,dict_emb['ws_list']))\n",
    "    \n",
    "        # Writing embedding dictionnary to disk\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(dict_emb, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        self.model.to(device)\n",
    "        self.loss_list = [x.to(device) for x in self.loss_list]\n",
    "    \n",
    " \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n",
      "Generating pairs\n",
      "Generating key_pairs\n",
      "finished creating key_pairs\n"
     ]
    }
   ],
   "source": [
    "# Snippet to test changes on very small dataset\n",
    "ds = wDataSet(enwik9_without_outliers_sentences,sampling_dict=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "batch_size = 41 || processed_batches = 12596/12597Time:  00:22:06.5145\n",
      "1 epoch of 20\n",
      " 12596 12597 batches, pairs 128165241, cum loss: 31768.68555\n",
      "Current score on wordsim Task: 0.60260923588538\n",
      "batch_size = 166814 || processed_batches = 57/12597Time:  00:00:38.85\r"
     ]
    }
   ],
   "source": [
    "# enwik9 min=5 max=1500\n",
    "ws_lists=[]\n",
    "for i in range(20):\n",
    "    w2v = W2V(ds, dim=100, neg_samples=10, alpha=0.005,iterations=20)\n",
    "    w2v.train_with_loader()\n",
    "    ws_lists.append(w2v.ws_list)\n",
    "np.mean(ws_lists,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_lists[0] = ws_lists[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4970557 , 0.61919465, 0.63405168, 0.6407982 , 0.6430412 ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ws_lists,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:19.71731\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3447.54932\n",
      "Current score on wordsim Task: 0.6565574518341265\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6565574518341265] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:19.18556\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3449.37134\n",
      "Current score on wordsim Task: 0.6592075674147663\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6592075674147663] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.95327\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3443.94019\n",
      "Current score on wordsim Task: 0.6647480229002757\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6647480229002757] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:18.56562\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3440.30127\n",
      "Current score on wordsim Task: 0.6756569995901194\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6756569995901194] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:18.32967\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3449.72754\n",
      "Current score on wordsim Task: 0.5750505871533997\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5750505871533997] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.60891\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3448.35205\n",
      "Current score on wordsim Task: 0.6765799781148302\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6765799781148302] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:18.78580\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3440.69922\n",
      "Current score on wordsim Task: 0.5775460153357479\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5775460153357479] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:20.74392\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3444.71680\n",
      "Current score on wordsim Task: 0.697870880269931\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.697870880269931] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.07405\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3446.45776\n",
      "Current score on wordsim Task: 0.5851029827581724\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5851029827581724] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:19.86175\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3454.34131\n",
      "Current score on wordsim Task: 0.7251126217872985\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.7251126217872985] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.49535\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3451.46021\n",
      "Current score on wordsim Task: 0.7161699769741046\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.7161699769741046] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.09095\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3445.46729\n",
      "Current score on wordsim Task: 0.6130964466892731\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6130964466892731] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:19.71868\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3443.73364\n",
      "Current score on wordsim Task: 0.7316645889887009\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.7316645889887009] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:18.16218\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3447.01294\n",
      "Current score on wordsim Task: 0.6258278650659588\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6258278650659588] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:14.20981\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3446.40796\n",
      "Current score on wordsim Task: 0.7098548916938794\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.7098548916938794] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.49809\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3441.81567\n",
      "Current score on wordsim Task: 0.6792412115788703\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6792412115788703] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.26816\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3441.21973\n",
      "Current score on wordsim Task: 0.6600925830675058\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6600925830675058] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.87344\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3443.25732\n",
      "Current score on wordsim Task: 0.6690978900515901\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6690978900515901] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.48567\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3446.80737\n",
      "Current score on wordsim Task: 0.5439003898560049\n",
      "ntm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5439003898560049] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.96333\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3447.07202\n",
      "Current score on wordsim Task: 0.7055303480537239\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.7055303480537239] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.49557\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3452.24219\n",
      "Current score on wordsim Task: 0.6588032514253978\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6588032514253978] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.71771\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3451.94531\n",
      "Current score on wordsim Task: 0.6845658755005198\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6845658755005198] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:13.18093\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3441.48657\n",
      "Current score on wordsim Task: 0.5811086506105344\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5811086506105344] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.52896\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3445.97900\n",
      "Current score on wordsim Task: 0.6430064159106205\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6430064159106205] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:14.92912\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3446.05127\n",
      "Current score on wordsim Task: 0.6152365723501104\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6152365723501104] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.23090\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3447.58960\n",
      "Current score on wordsim Task: 0.6309185918248119\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6309185918248119] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.73959\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3445.62988\n",
      "Current score on wordsim Task: 0.680657425015266\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.680657425015266] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.98372\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3444.92480\n",
      "Current score on wordsim Task: 0.6587267975419936\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6587267975419936] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:14.81897\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3447.18896\n",
      "Current score on wordsim Task: 0.6597462790143827\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6597462790143827] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.19653\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3446.01440\n",
      "Current score on wordsim Task: 0.6215696716013854\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6215696716013854] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.08565\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3442.29541\n",
      "Current score on wordsim Task: 0.6978489360522292\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6978489360522292] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.38445\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3445.78540\n",
      "Current score on wordsim Task: 0.5928485232598751\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5928485232598751] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.01793\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3444.87354\n",
      "Current score on wordsim Task: 0.5930377134364013\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5930377134364013] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.32899\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3448.83398\n",
      "Current score on wordsim Task: 0.6422783624684584\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6422783624684584] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.52891\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3445.40820\n",
      "Current score on wordsim Task: 0.6360718050993864\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6360718050993864] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:16.34122\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3446.57495\n",
      "Current score on wordsim Task: 0.5706356488850435\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5706356488850435] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:18.22991\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3448.41406\n",
      "Current score on wordsim Task: 0.6550912532397968\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6550912532397968] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:15.06101\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3445.85547\n",
      "Current score on wordsim Task: 0.5774645961384776\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.5774645961384776] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.84115\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3440.92871\n",
      "Current score on wordsim Task: 0.7156313977140185\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.7156313977140185] \n",
      "cuda:0\n",
      "starting training\n",
      "batch_size = 3 || processed_batches = 1254/1255Time:  00:01:17.56221\n",
      "1 epoch of 1\n",
      " 1254 1255 batches, pairs 13006091, cum loss: 3444.54224\n",
      "Current score on wordsim Task: 0.6649938339250362\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.0055_dim100_workers2_ctxw5_neg_samples10_use_cudaTrue_iterations1.pkl to disk with ws_score: [0.6649938339250362] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.64820377])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#w/o outliers\n",
    "ws_lists=[]\n",
    "for i in range(40):\n",
    "    w2v = W2V(ds, neg_samples=10, alpha=0.0055,iterations=1)\n",
    "    w2v.train_with_loader()\n",
    "    ws_lists.append(w2v.ws_list)\n",
    "np.mean(ws_lists,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6649938339250362]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.ws_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.62997657, 0.53193329, 0.51931784, 0.51932677, 0.56761835,\n",
       "       0.57605663, 0.54397289, 0.53736842, 0.49581075, 0.53589549])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ws_lists,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15076,\n",
       " 163791,\n",
       " 18176,\n",
       " 59814,\n",
       " 248847,\n",
       " 225957,\n",
       " 143512,\n",
       " 46287,\n",
       " 205140,\n",
       " 150878,\n",
       " 102846,\n",
       " 67943,\n",
       " 27324,\n",
       " 174496,\n",
       " 73856,\n",
       " 212638,\n",
       " 62243,\n",
       " 18176,\n",
       " 136516,\n",
       " 160797,\n",
       " 22239,\n",
       " 63229,\n",
       " 47775,\n",
       " 18176,\n",
       " 194922,\n",
       " 12073,\n",
       " 140246,\n",
       " 62243,\n",
       " 18176,\n",
       " 248847,\n",
       " 134165,\n",
       " 76914,\n",
       " 1896,\n",
       " 121728,\n",
       " 82277,\n",
       " 209876,\n",
       " 116376,\n",
       " 221460,\n",
       " 46197,\n",
       " 96145,\n",
       " 248847,\n",
       " 177468,\n",
       " 18176,\n",
       " 57825,\n",
       " 231796,\n",
       " 248847,\n",
       " 94133,\n",
       " 234719,\n",
       " 179612,\n",
       " 126777,\n",
       " 249401,\n",
       " 77282,\n",
       " 67102,\n",
       " 37985,\n",
       " 42066,\n",
       " 49716,\n",
       " 18176,\n",
       " 245592,\n",
       " 219133,\n",
       " 148982,\n",
       " 212054,\n",
       " 68472,\n",
       " 18176,\n",
       " 117826,\n",
       " 12073,\n",
       " 138326,\n",
       " 64035,\n",
       " 155458,\n",
       " 201939,\n",
       " 186399,\n",
       " 155269,\n",
       " 134379,\n",
       " 248847,\n",
       " 120801,\n",
       " 196087,\n",
       " 16465,\n",
       " 154173,\n",
       " 18176,\n",
       " 61974,\n",
       " 248847,\n",
       " 125733,\n",
       " 215770,\n",
       " 201430,\n",
       " 201939,\n",
       " 14777,\n",
       " 103394,\n",
       " 180462,\n",
       " 67102,\n",
       " 174761,\n",
       " 55817,\n",
       " 79101,\n",
       " 154173,\n",
       " 73903,\n",
       " 188062,\n",
       " 177689,\n",
       " 111521,\n",
       " 6665,\n",
       " 248847,\n",
       " 116592,\n",
       " 49160,\n",
       " 128260,\n",
       " 41673,\n",
       " 140588,\n",
       " 148052,\n",
       " 84036,\n",
       " 177689,\n",
       " 8885,\n",
       " 67943,\n",
       " 169347,\n",
       " 95847,\n",
       " 154173,\n",
       " 126777,\n",
       " 90694,\n",
       " 112859,\n",
       " 210601,\n",
       " 178285,\n",
       " 18176,\n",
       " 128829,\n",
       " 85235,\n",
       " 18176,\n",
       " 144715,\n",
       " 179550,\n",
       " 154173,\n",
       " 31658,\n",
       " 24061,\n",
       " 154173,\n",
       " 85363,\n",
       " 76377,\n",
       " 177740,\n",
       " 25779,\n",
       " 22971,\n",
       " 93594,\n",
       " 94450,\n",
       " 18176,\n",
       " 66736,\n",
       " 159845,\n",
       " 177689,\n",
       " 1896,\n",
       " 16220,\n",
       " 30795,\n",
       " 177689,\n",
       " 246583,\n",
       " 199850,\n",
       " 162628,\n",
       " 121020,\n",
       " 248847,\n",
       " 177740,\n",
       " 96514,\n",
       " 175827,\n",
       " 103394,\n",
       " 25487,\n",
       " 18176,\n",
       " 136288,\n",
       " 177740,\n",
       " 43814,\n",
       " 138326,\n",
       " 126666,\n",
       " 187866,\n",
       " 40342,\n",
       " 219133,\n",
       " 18176,\n",
       " 194616,\n",
       " 203191,\n",
       " 186399,\n",
       " 172927,\n",
       " 226194,\n",
       " 194616,\n",
       " 223401,\n",
       " 12073,\n",
       " 134165,\n",
       " 34688,\n",
       " 210693,\n",
       " 141129,\n",
       " 209876,\n",
       " 221460,\n",
       " 177740,\n",
       " 159847,\n",
       " 187186,\n",
       " 46964,\n",
       " 31185,\n",
       " 67943,\n",
       " 201875,\n",
       " 227560,\n",
       " 168332,\n",
       " 23102,\n",
       " 177740,\n",
       " 126777,\n",
       " 252306,\n",
       " 36192,\n",
       " 203055,\n",
       " 138326,\n",
       " 18176,\n",
       " 537,\n",
       " 122107,\n",
       " 204355,\n",
       " 87135,\n",
       " 198489,\n",
       " 537,\n",
       " 537,\n",
       " 186916,\n",
       " 18176,\n",
       " 223401,\n",
       " 147810,\n",
       " 121020,\n",
       " 40197,\n",
       " 105721,\n",
       " 18176,\n",
       " 188026,\n",
       " 114998,\n",
       " 236483,\n",
       " 226946,\n",
       " 37446,\n",
       " 131535,\n",
       " 537,\n",
       " 151126,\n",
       " 126777,\n",
       " 10646,\n",
       " 173638,\n",
       " 154289,\n",
       " 154173,\n",
       " 164027,\n",
       " 58911,\n",
       " 248847,\n",
       " 6308,\n",
       " 248847,\n",
       " 58615,\n",
       " 58688,\n",
       " 198022,\n",
       " 147976,\n",
       " 243500,\n",
       " 11711,\n",
       " 221460,\n",
       " 248847,\n",
       " 61803,\n",
       " 140724,\n",
       " 2507,\n",
       " 48570,\n",
       " 140724,\n",
       " 89655,\n",
       " 100912,\n",
       " 177689,\n",
       " 102023,\n",
       " 12073,\n",
       " 177689,\n",
       " 240884,\n",
       " 136957,\n",
       " 58615,\n",
       " 77248,\n",
       " 186977,\n",
       " 76870,\n",
       " 80958,\n",
       " 98188,\n",
       " 121020,\n",
       " 24611,\n",
       " 131242,\n",
       " 34688,\n",
       " 114133,\n",
       " 12073,\n",
       " 143181,\n",
       " 18176,\n",
       " 126777,\n",
       " 17982,\n",
       " 112760,\n",
       " 67102,\n",
       " 73688,\n",
       " 177689,\n",
       " 53717,\n",
       " 16840,\n",
       " 196432,\n",
       " 154173,\n",
       " 30795,\n",
       " 200522,\n",
       " 126777,\n",
       " 24819,\n",
       " 76377,\n",
       " 196780,\n",
       " 40501,\n",
       " 92314,\n",
       " 32997,\n",
       " 18176,\n",
       " 140387,\n",
       " 147976,\n",
       " 138326,\n",
       " 248847,\n",
       " 30458,\n",
       " 126777,\n",
       " 216971,\n",
       " 1896,\n",
       " 58016,\n",
       " 67117,\n",
       " 18176,\n",
       " 253272,\n",
       " 12657,\n",
       " 221460,\n",
       " 63229,\n",
       " 32370,\n",
       " 108511,\n",
       " 18176,\n",
       " 71367,\n",
       " 138943,\n",
       " 68014,\n",
       " 96514,\n",
       " 248847,\n",
       " 49178,\n",
       " 76160,\n",
       " 126777,\n",
       " 115355,\n",
       " 92433,\n",
       " 248847,\n",
       " 32349,\n",
       " 205253,\n",
       " 25779,\n",
       " 12657,\n",
       " 18176,\n",
       " 18176,\n",
       " 190210,\n",
       " 177793,\n",
       " 48180,\n",
       " 248847,\n",
       " 112352,\n",
       " 154173,\n",
       " 81140,\n",
       " 154173,\n",
       " 126777,\n",
       " 150128,\n",
       " 199477,\n",
       " 168603,\n",
       " 1896,\n",
       " 140588,\n",
       " 177689,\n",
       " 148763,\n",
       " 52388,\n",
       " 18176,\n",
       " 151024,\n",
       " 12657,\n",
       " 248847,\n",
       " 134165,\n",
       " 63229,\n",
       " 76562,\n",
       " 12657,\n",
       " 67531,\n",
       " 177689,\n",
       " 203202,\n",
       " 35393,\n",
       " 35980,\n",
       " 199551,\n",
       " 87307,\n",
       " 42299,\n",
       " 108662,\n",
       " 27400,\n",
       " 183419,\n",
       " 179668,\n",
       " 180176,\n",
       " 126777,\n",
       " 118477,\n",
       " 200125,\n",
       " 179781,\n",
       " 154173,\n",
       " 128241,\n",
       " 161610,\n",
       " 177689,\n",
       " 116376,\n",
       " 12073,\n",
       " 126777,\n",
       " 141129,\n",
       " 68119,\n",
       " 102023,\n",
       " 18176,\n",
       " 501,\n",
       " 79266,\n",
       " 110328,\n",
       " 39783,\n",
       " 107482,\n",
       " 40623,\n",
       " 248847,\n",
       " 113300,\n",
       " 248847,\n",
       " 89940,\n",
       " 106146,\n",
       " 186298,\n",
       " 161583,\n",
       " 248847,\n",
       " 110328,\n",
       " 64441,\n",
       " 248847,\n",
       " 110328,\n",
       " 106146,\n",
       " 219463,\n",
       " 93022,\n",
       " 14777,\n",
       " 18176,\n",
       " 14656,\n",
       " 184539,\n",
       " 48704,\n",
       " 223401,\n",
       " 98188,\n",
       " 77123,\n",
       " 141129,\n",
       " 180228,\n",
       " 209712,\n",
       " 34047,\n",
       " 226138,\n",
       " 128260,\n",
       " 40940,\n",
       " 87307,\n",
       " 40623,\n",
       " 111143,\n",
       " 27824,\n",
       " 226946,\n",
       " 110643,\n",
       " 103394,\n",
       " 154173,\n",
       " 240442,\n",
       " 212638,\n",
       " 14656,\n",
       " 67943,\n",
       " 53181,\n",
       " 214441,\n",
       " 38173,\n",
       " 186916,\n",
       " 34170,\n",
       " 240105,\n",
       " 55729,\n",
       " 226138,\n",
       " 126714,\n",
       " 223401,\n",
       " 80958,\n",
       " 132208,\n",
       " 248847,\n",
       " 162628,\n",
       " 18176,\n",
       " 66736,\n",
       " 66082,\n",
       " 151661,\n",
       " 76377,\n",
       " 205253,\n",
       " 181139,\n",
       " 177689,\n",
       " 11969,\n",
       " 96283,\n",
       " 36833,\n",
       " 116376,\n",
       " 86354,\n",
       " 134379,\n",
       " 223401,\n",
       " 116674,\n",
       " 54656,\n",
       " 184539,\n",
       " 12777,\n",
       " 236551,\n",
       " 240105,\n",
       " 41327,\n",
       " 38429,\n",
       " 18176,\n",
       " 233227,\n",
       " 246691,\n",
       " 116376,\n",
       " 157460,\n",
       " 30444,\n",
       " 217026,\n",
       " 198062,\n",
       " 18176,\n",
       " 53545,\n",
       " 248847,\n",
       " 98188,\n",
       " 208927,\n",
       " 82386,\n",
       " 2929,\n",
       " 197884,\n",
       " 134414,\n",
       " 154173,\n",
       " 134165,\n",
       " 3230,\n",
       " 115986,\n",
       " 154173,\n",
       " 196432,\n",
       " 226194,\n",
       " 105941,\n",
       " 126777,\n",
       " 18176,\n",
       " 226138,\n",
       " 861,\n",
       " 240167,\n",
       " 132212,\n",
       " 248847,\n",
       " 154173,\n",
       " 252706,\n",
       " 145118,\n",
       " 145118,\n",
       " 18176,\n",
       " 236450,\n",
       " 108819,\n",
       " 177689,\n",
       " 14656,\n",
       " 138976,\n",
       " 58688,\n",
       " 154173,\n",
       " 78503,\n",
       " 61803,\n",
       " 248847,\n",
       " 157128,\n",
       " 71338,\n",
       " 155111,\n",
       " 143081,\n",
       " 117352,\n",
       " 140397,\n",
       " 9805,\n",
       " 237808,\n",
       " 14656,\n",
       " 30795,\n",
       " 242790,\n",
       " 73903,\n",
       " 236551,\n",
       " 223401,\n",
       " 150481,\n",
       " 14656,\n",
       " 24062,\n",
       " 108819,\n",
       " 18176,\n",
       " 67943,\n",
       " 108819,\n",
       " 29668,\n",
       " 14656,\n",
       " 171384,\n",
       " 226138,\n",
       " 98188,\n",
       " 53545,\n",
       " 34688,\n",
       " 12073,\n",
       " 147034,\n",
       " 68472,\n",
       " 111912,\n",
       " 244899,\n",
       " 193448,\n",
       " 29187,\n",
       " 18176,\n",
       " 73927,\n",
       " 222292,\n",
       " 243791,\n",
       " 177689,\n",
       " 861,\n",
       " 9805,\n",
       " 226876,\n",
       " 22737,\n",
       " 181303,\n",
       " 150894,\n",
       " 37486,\n",
       " 240105,\n",
       " 18176,\n",
       " 215150,\n",
       " 221796,\n",
       " 14656,\n",
       " 12073,\n",
       " 108819,\n",
       " 200908,\n",
       " 102023,\n",
       " 18176,\n",
       " 501,\n",
       " 221734,\n",
       " 186977,\n",
       " 224382,\n",
       " 80958,\n",
       " 58688,\n",
       " 102023,\n",
       " 159845,\n",
       " 147709,\n",
       " 1896,\n",
       " 77986,\n",
       " 134165,\n",
       " 212010,\n",
       " 160263,\n",
       " 249668,\n",
       " 140588,\n",
       " 138326,\n",
       " 47775,\n",
       " 160639,\n",
       " 126777,\n",
       " 241955,\n",
       " 18176,\n",
       " 181816,\n",
       " 210601,\n",
       " 249019,\n",
       " 87307,\n",
       " 12106,\n",
       " 248847,\n",
       " 63229,\n",
       " 58688,\n",
       " 111421,\n",
       " 235671,\n",
       " 24013,\n",
       " 204034,\n",
       " 215724,\n",
       " 180462,\n",
       " 111521,\n",
       " 215265,\n",
       " 214184,\n",
       " 40342,\n",
       " 154173,\n",
       " 188867,\n",
       " 248847,\n",
       " 248847,\n",
       " 221857,\n",
       " 92340,\n",
       " 248847,\n",
       " 122344,\n",
       " 21924,\n",
       " 18176,\n",
       " 145979,\n",
       " 17015,\n",
       " 134923,\n",
       " 140588,\n",
       " 40342,\n",
       " 58688,\n",
       " 248847,\n",
       " 76377,\n",
       " 138326,\n",
       " 103394,\n",
       " 129416,\n",
       " 126777,\n",
       " 166560,\n",
       " 229855,\n",
       " 216861,\n",
       " 248847,\n",
       " 76377,\n",
       " 234485,\n",
       " 216123,\n",
       " 216595,\n",
       " 34047,\n",
       " 18176,\n",
       " 223401,\n",
       " 51572,\n",
       " 109766,\n",
       " 40342,\n",
       " 51902,\n",
       " 110772,\n",
       " 24611,\n",
       " 84947,\n",
       " 78337,\n",
       " 250237,\n",
       " 154906,\n",
       " 223401,\n",
       " 170822,\n",
       " 200681,\n",
       " 76904,\n",
       " 70601,\n",
       " 18176,\n",
       " 18176,\n",
       " 248847,\n",
       " 49649,\n",
       " 248532,\n",
       " 223401,\n",
       " 206839,\n",
       " 34047,\n",
       " 56676,\n",
       " 177689,\n",
       " 18176,\n",
       " 65049,\n",
       " 248847,\n",
       " 96230,\n",
       " 68472,\n",
       " 248847,\n",
       " 117826,\n",
       " 194106,\n",
       " 132915,\n",
       " 183506,\n",
       " 54656,\n",
       " 154173,\n",
       " 175507,\n",
       " 148201,\n",
       " 146610,\n",
       " 212054,\n",
       " 149024,\n",
       " 138326,\n",
       " 65231,\n",
       " 76377,\n",
       " 100249,\n",
       " 76894,\n",
       " 18176,\n",
       " 153723,\n",
       " 58016,\n",
       " 182438,\n",
       " 177689,\n",
       " 31705,\n",
       " 154173,\n",
       " 18176,\n",
       " 25302,\n",
       " 12073,\n",
       " 202865,\n",
       " 67943,\n",
       " 159845,\n",
       " 94450,\n",
       " 24547,\n",
       " 233464,\n",
       " 54141,\n",
       " 252624,\n",
       " 171345,\n",
       " 18176,\n",
       " 161583,\n",
       " 65750,\n",
       " 40342,\n",
       " 71706,\n",
       " 219463,\n",
       " 211681,\n",
       " 155136,\n",
       " 18176,\n",
       " 75900,\n",
       " 243639,\n",
       " 68119,\n",
       " 232611,\n",
       " 45410,\n",
       " 122714,\n",
       " 89403,\n",
       " 162863,\n",
       " 48344,\n",
       " 204081,\n",
       " 180462,\n",
       " 168794,\n",
       " 73595,\n",
       " 172495,\n",
       " 12777,\n",
       " 224538,\n",
       " 161624,\n",
       " 130603,\n",
       " 128266,\n",
       " 244418,\n",
       " 154173,\n",
       " 170719,\n",
       " 206814,\n",
       " 196432,\n",
       " 245249,\n",
       " 151024,\n",
       " 186399,\n",
       " 3418,\n",
       " 248847,\n",
       " 128829,\n",
       " 240105,\n",
       " 18176,\n",
       " 188695,\n",
       " 12073,\n",
       " 248847,\n",
       " 109766,\n",
       " 155870,\n",
       " 201445,\n",
       " 135990,\n",
       " 2507,\n",
       " 18176,\n",
       " 229660,\n",
       " 201445,\n",
       " 140588,\n",
       " 230829,\n",
       " 210458,\n",
       " 177689,\n",
       " 62446,\n",
       " 126777,\n",
       " 18176,\n",
       " 248532,\n",
       " 157404,\n",
       " 201445,\n",
       " 57033,\n",
       " 107691,\n",
       " 154173,\n",
       " 205140,\n",
       " 43814,\n",
       " 210601,\n",
       " 62446,\n",
       " 18176,\n",
       " 155136,\n",
       " 248847,\n",
       " 134165,\n",
       " 81725,\n",
       " 18176,\n",
       " 210601,\n",
       " 205140,\n",
       " 248847,\n",
       " 43814,\n",
       " 157404,\n",
       " 58688,\n",
       " 38456,\n",
       " 157404,\n",
       " 196951,\n",
       " 18176,\n",
       " 75179,\n",
       " 244871,\n",
       " 161365,\n",
       " 75179,\n",
       " 202351,\n",
       " 230200,\n",
       " 177689,\n",
       " 62212,\n",
       " 134165,\n",
       " 117939,\n",
       " 154173,\n",
       " 225957,\n",
       " 72910,\n",
       " 18176,\n",
       " 111912,\n",
       " 248847,\n",
       " 116828,\n",
       " 102023,\n",
       " 155458,\n",
       " 27415,\n",
       " 18176,\n",
       " 243500,\n",
       " 172927,\n",
       " 135661,\n",
       " 54656,\n",
       " 58870,\n",
       " 76377,\n",
       " 215069,\n",
       " 240105,\n",
       " 85622,\n",
       " 177689,\n",
       " 12073,\n",
       " 41058,\n",
       " 34688,\n",
       " 197703,\n",
       " 142242,\n",
       " 230829,\n",
       " 133785,\n",
       " 20358,\n",
       " 185203,\n",
       " 67295,\n",
       " 126777,\n",
       " 23091,\n",
       " 99421,\n",
       " 28910,\n",
       " 130603,\n",
       " 10190,\n",
       " 4899,\n",
       " 18176,\n",
       " 153488,\n",
       " 227376,\n",
       " 60798,\n",
       " 54141,\n",
       " 240547,\n",
       " 167152,\n",
       " 154173,\n",
       " 86149,\n",
       " 58688,\n",
       " 18176,\n",
       " 14777,\n",
       " 248847,\n",
       " 236483,\n",
       " 173287,\n",
       " 240421,\n",
       " 108872,\n",
       " 83318,\n",
       " 236551,\n",
       " 126777,\n",
       " 26753,\n",
       " 230471,\n",
       " 42025,\n",
       " 40501,\n",
       " 34047,\n",
       " 177689,\n",
       " 248847,\n",
       " 177944,\n",
       " 233464,\n",
       " 225367,\n",
       " 158344,\n",
       " 37726,\n",
       " 126777,\n",
       " 236351,\n",
       " 53013,\n",
       " 66736,\n",
       " 43334,\n",
       " 124857,\n",
       " 187186,\n",
       " 66736,\n",
       " 150855,\n",
       " 161009,\n",
       " 223401,\n",
       " 226782,\n",
       " 243500,\n",
       " 177689,\n",
       " 128260,\n",
       " 120379,\n",
       " 58688,\n",
       " 130603,\n",
       " 113313,\n",
       " 67943,\n",
       " 188026,\n",
       " 122079,\n",
       " 18176,\n",
       " 112507,\n",
       " 147661,\n",
       " 174496,\n",
       " 52831,\n",
       " 164142,\n",
       " 175351,\n",
       " 248847,\n",
       " 125607,\n",
       " 18176,\n",
       " 68472,\n",
       " 224827,\n",
       " 238834,\n",
       " 248847,\n",
       " 53545,\n",
       " 123497,\n",
       " 51902,\n",
       " 109505,\n",
       " 40342,\n",
       " 40342,\n",
       " 225104,\n",
       " 90518,\n",
       " 18176,\n",
       " 223401,\n",
       " 117155,\n",
       " 75179,\n",
       " 18176,\n",
       " 122903,\n",
       " 131669,\n",
       " 3839,\n",
       " 177689,\n",
       " 244339,\n",
       " 34571,\n",
       " 67943,\n",
       " 204385,\n",
       " 177689,\n",
       " 134980,\n",
       " 97103,\n",
       " 67943,\n",
       " 87121,\n",
       " 248847,\n",
       " 105721,\n",
       " 153691,\n",
       " 67943,\n",
       " 1896,\n",
       " 68472,\n",
       " 187884,\n",
       " 121363,\n",
       " 67943,\n",
       " 145955,\n",
       " 60654,\n",
       " 3111,\n",
       " 152701,\n",
       " 223401,\n",
       " 37482,\n",
       " 143406,\n",
       " 170012,\n",
       " 18176,\n",
       " 248847,\n",
       " 131160,\n",
       " 18176,\n",
       " 27369,\n",
       " 103492,\n",
       " 104739,\n",
       " 124081,\n",
       " 248847,\n",
       " 115671,\n",
       " 134269,\n",
       " 166911,\n",
       " 3392,\n",
       " 248847,\n",
       " 108511,\n",
       " 59071,\n",
       " 18176,\n",
       " 215724,\n",
       " 167771,\n",
       " 80892,\n",
       " 248847,\n",
       " 240105,\n",
       " 154173,\n",
       " 248847,\n",
       " 2697,\n",
       " 169750,\n",
       " 190928,\n",
       " 76377,\n",
       " 63229,\n",
       " 248847,\n",
       " 120070,\n",
       " 25410,\n",
       " 110328,\n",
       " 138326,\n",
       " 151241,\n",
       " 54656,\n",
       " 43814,\n",
       " 18176,\n",
       " 42834,\n",
       " 248847,\n",
       " 177689,\n",
       " 94849,\n",
       " 87229,\n",
       " 9896,\n",
       " 8404,\n",
       " 63594,\n",
       " 18176,\n",
       " 49126,\n",
       " 94521,\n",
       " 202865,\n",
       " 126777,\n",
       " 105530,\n",
       " 18176,\n",
       " 134980,\n",
       " 94388,\n",
       " 18176,\n",
       " 42025,\n",
       " 126777,\n",
       " 203432,\n",
       " 94521,\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_key = max(ds.pair_dict, key= lambda x: len(set(ds.pair_dict[x])))\n",
    "ds.pair_dict.pop(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: [182957,\n",
       "              173256,\n",
       "              214898,\n",
       "              225058,\n",
       "              93299,\n",
       "              123383,\n",
       "              24659,\n",
       "              196012,\n",
       "              214898,\n",
       "              204236],\n",
       "             1: [203692,\n",
       "              241626,\n",
       "              185328,\n",
       "              201277,\n",
       "              148684,\n",
       "              190353,\n",
       "              198067,\n",
       "              190353,\n",
       "              69623,\n",
       "              220725,\n",
       "              185328,\n",
       "              126633,\n",
       "              145544,\n",
       "              190353,\n",
       "              119668,\n",
       "              1361,\n",
       "              220725,\n",
       "              145605,\n",
       "              145605,\n",
       "              220725,\n",
       "              104557,\n",
       "              147976,\n",
       "              31738,\n",
       "              101724,\n",
       "              151246,\n",
       "              42602,\n",
       "              125183,\n",
       "              58361,\n",
       "              220725,\n",
       "              1361,\n",
       "              28283,\n",
       "              23082,\n",
       "              100824,\n",
       "              1361,\n",
       "              42681,\n",
       "              209297,\n",
       "              220725,\n",
       "              117624,\n",
       "              222975,\n",
       "              243585,\n",
       "              209840,\n",
       "              139219,\n",
       "              139458,\n",
       "              139219,\n",
       "              163877,\n",
       "              139219,\n",
       "              193669,\n",
       "              202715,\n",
       "              108637,\n",
       "              135792,\n",
       "              158977,\n",
       "              10863,\n",
       "              217361,\n",
       "              230871,\n",
       "              243585,\n",
       "              220725,\n",
       "              81496,\n",
       "              193669,\n",
       "              7193,\n",
       "              163877,\n",
       "              100793,\n",
       "              161828,\n",
       "              236860,\n",
       "              81496,\n",
       "              182624,\n",
       "              193683,\n",
       "              196168,\n",
       "              215733,\n",
       "              80924,\n",
       "              3946],\n",
       "             2: [195765, 195334, 40484, 196798, 14469, 232392, 225009],\n",
       "             3: [144206,\n",
       "              102271,\n",
       "              136719,\n",
       "              93299,\n",
       "              137135,\n",
       "              227447,\n",
       "              212782,\n",
       "              163877,\n",
       "              150674,\n",
       "              112055],\n",
       "             4: [141021,\n",
       "              31449,\n",
       "              102130,\n",
       "              1596,\n",
       "              198067,\n",
       "              160630,\n",
       "              106350,\n",
       "              169712,\n",
       "              31738,\n",
       "              165971],\n",
       "             5: [123383,\n",
       "              93934,\n",
       "              217192,\n",
       "              163877,\n",
       "              181285,\n",
       "              172197,\n",
       "              202818,\n",
       "              131952,\n",
       "              241197,\n",
       "              198807],\n",
       "             6: [193669,\n",
       "              163877,\n",
       "              163877,\n",
       "              172197,\n",
       "              11735,\n",
       "              238064,\n",
       "              200313,\n",
       "              101896,\n",
       "              20497,\n",
       "              131846,\n",
       "              9718,\n",
       "              54726,\n",
       "              57138,\n",
       "              70358,\n",
       "              93299,\n",
       "              243585,\n",
       "              220725,\n",
       "              190353,\n",
       "              243585,\n",
       "              139219,\n",
       "              209840,\n",
       "              204346,\n",
       "              163877,\n",
       "              172197,\n",
       "              186491,\n",
       "              123383,\n",
       "              61839,\n",
       "              58063,\n",
       "              106580,\n",
       "              104926,\n",
       "              63810,\n",
       "              31738,\n",
       "              123383,\n",
       "              29881,\n",
       "              131050,\n",
       "              172197,\n",
       "              83355,\n",
       "              221246,\n",
       "              93299,\n",
       "              193669,\n",
       "              20574,\n",
       "              246493,\n",
       "              25620,\n",
       "              163877,\n",
       "              23835,\n",
       "              178624,\n",
       "              193669,\n",
       "              163877,\n",
       "              135792,\n",
       "              172197,\n",
       "              67793,\n",
       "              201277,\n",
       "              54726,\n",
       "              243585,\n",
       "              70415,\n",
       "              220725,\n",
       "              126120,\n",
       "              93299,\n",
       "              43525,\n",
       "              163877,\n",
       "              79126,\n",
       "              93299,\n",
       "              115667,\n",
       "              163877,\n",
       "              217774,\n",
       "              127682,\n",
       "              193669,\n",
       "              182598,\n",
       "              7173,\n",
       "              222339,\n",
       "              64316,\n",
       "              30662,\n",
       "              111810,\n",
       "              38293,\n",
       "              120438,\n",
       "              218492,\n",
       "              138677,\n",
       "              108361,\n",
       "              105479,\n",
       "              182048,\n",
       "              93299,\n",
       "              131402,\n",
       "              189905,\n",
       "              172197,\n",
       "              163877,\n",
       "              244915,\n",
       "              190032,\n",
       "              163877,\n",
       "              244990,\n",
       "              55994,\n",
       "              163877,\n",
       "              173232,\n",
       "              93299,\n",
       "              94689,\n",
       "              21506,\n",
       "              163842,\n",
       "              163877,\n",
       "              81095,\n",
       "              48695,\n",
       "              236046,\n",
       "              193669,\n",
       "              193669,\n",
       "              163877,\n",
       "              216189,\n",
       "              41768,\n",
       "              214115,\n",
       "              3646,\n",
       "              139841,\n",
       "              209297,\n",
       "              139219,\n",
       "              139219,\n",
       "              190353,\n",
       "              139219,\n",
       "              190353,\n",
       "              220143,\n",
       "              102909,\n",
       "              223273,\n",
       "              34566,\n",
       "              193669,\n",
       "              101450,\n",
       "              123383,\n",
       "              118882,\n",
       "              92184,\n",
       "              193669,\n",
       "              246493,\n",
       "              102568,\n",
       "              145667,\n",
       "              220725,\n",
       "              18070,\n",
       "              139219,\n",
       "              65222,\n",
       "              52598,\n",
       "              87301,\n",
       "              138677,\n",
       "              9478,\n",
       "              250087,\n",
       "              1987,\n",
       "              233123,\n",
       "              93299,\n",
       "              25620,\n",
       "              244990,\n",
       "              193669,\n",
       "              163877,\n",
       "              163877,\n",
       "              246967,\n",
       "              183643,\n",
       "              138503,\n",
       "              135409,\n",
       "              177113,\n",
       "              163877,\n",
       "              93299,\n",
       "              193669,\n",
       "              249277,\n",
       "              169770,\n",
       "              25620,\n",
       "              89652,\n",
       "              56380,\n",
       "              134421,\n",
       "              243585,\n",
       "              221256,\n",
       "              1516,\n",
       "              161828,\n",
       "              176239,\n",
       "              163877,\n",
       "              93299,\n",
       "              89716,\n",
       "              175304,\n",
       "              58063,\n",
       "              40,\n",
       "              193669,\n",
       "              193669,\n",
       "              163877,\n",
       "              105479,\n",
       "              193669,\n",
       "              232122,\n",
       "              37728,\n",
       "              77093,\n",
       "              60803,\n",
       "              231637,\n",
       "              163877,\n",
       "              224211,\n",
       "              139219,\n",
       "              220725,\n",
       "              42980,\n",
       "              137958,\n",
       "              192880,\n",
       "              48250,\n",
       "              188036,\n",
       "              172197,\n",
       "              206643,\n",
       "              25620,\n",
       "              83215,\n",
       "              219633,\n",
       "              138677,\n",
       "              62620,\n",
       "              54726,\n",
       "              112055,\n",
       "              164450,\n",
       "              58449,\n",
       "              207549,\n",
       "              41590,\n",
       "              25620,\n",
       "              138523,\n",
       "              65222,\n",
       "              92184,\n",
       "              161828,\n",
       "              243585,\n",
       "              87301,\n",
       "              42681,\n",
       "              243585,\n",
       "              163877,\n",
       "              139219,\n",
       "              3646,\n",
       "              130124,\n",
       "              243585,\n",
       "              173974,\n",
       "              131846,\n",
       "              177217,\n",
       "              123588,\n",
       "              14958,\n",
       "              119989,\n",
       "              50428,\n",
       "              112055,\n",
       "              243585,\n",
       "              155679,\n",
       "              179781,\n",
       "              226720,\n",
       "              208768,\n",
       "              243953,\n",
       "              232122,\n",
       "              176239,\n",
       "              123383,\n",
       "              164808,\n",
       "              163877,\n",
       "              248972,\n",
       "              163877,\n",
       "              106580,\n",
       "              195355,\n",
       "              138677,\n",
       "              243585,\n",
       "              31881,\n",
       "              163877,\n",
       "              176239,\n",
       "              179781,\n",
       "              126279,\n",
       "              88994,\n",
       "              235633,\n",
       "              161828,\n",
       "              155679,\n",
       "              229004,\n",
       "              226720,\n",
       "              7173,\n",
       "              25897,\n",
       "              93299,\n",
       "              220725,\n",
       "              208335,\n",
       "              42681,\n",
       "              169421,\n",
       "              18070,\n",
       "              117325,\n",
       "              175304,\n",
       "              163877,\n",
       "              179781,\n",
       "              140584,\n",
       "              198398,\n",
       "              138677,\n",
       "              163877,\n",
       "              190984,\n",
       "              93299,\n",
       "              25897,\n",
       "              208335,\n",
       "              196024,\n",
       "              99498,\n",
       "              123383,\n",
       "              174619,\n",
       "              204346,\n",
       "              163877,\n",
       "              150941,\n",
       "              25620,\n",
       "              93330,\n",
       "              112055,\n",
       "              93299,\n",
       "              163877,\n",
       "              198067,\n",
       "              193669,\n",
       "              154265,\n",
       "              135792,\n",
       "              106059,\n",
       "              163877,\n",
       "              224211,\n",
       "              68328,\n",
       "              193669,\n",
       "              163877,\n",
       "              171558,\n",
       "              115667,\n",
       "              168768,\n",
       "              167349,\n",
       "              100828,\n",
       "              196703,\n",
       "              93299,\n",
       "              112293,\n",
       "              54783,\n",
       "              93299,\n",
       "              92184,\n",
       "              129969,\n",
       "              193669,\n",
       "              204346,\n",
       "              243585,\n",
       "              53217,\n",
       "              134837,\n",
       "              58063,\n",
       "              58063,\n",
       "              163877,\n",
       "              81483,\n",
       "              126744,\n",
       "              163877,\n",
       "              7173,\n",
       "              48695,\n",
       "              9406,\n",
       "              193669,\n",
       "              25897,\n",
       "              49236,\n",
       "              172197,\n",
       "              190032,\n",
       "              163877,\n",
       "              106580,\n",
       "              1361,\n",
       "              34566,\n",
       "              102909,\n",
       "              25620,\n",
       "              106580,\n",
       "              229457,\n",
       "              101231,\n",
       "              163877,\n",
       "              4506,\n",
       "              234421,\n",
       "              161828,\n",
       "              151849,\n",
       "              224211,\n",
       "              233123,\n",
       "              126744,\n",
       "              25620,\n",
       "              234421,\n",
       "              17518,\n",
       "              172012,\n",
       "              144021,\n",
       "              46688,\n",
       "              12171,\n",
       "              138677,\n",
       "              222260,\n",
       "              25620,\n",
       "              172012,\n",
       "              70232,\n",
       "              92184,\n",
       "              25620,\n",
       "              155489,\n",
       "              20497,\n",
       "              25620,\n",
       "              248728,\n",
       "              163877,\n",
       "              111738,\n",
       "              92184,\n",
       "              125914,\n",
       "              138677,\n",
       "              170823,\n",
       "              38477,\n",
       "              172197,\n",
       "              21700,\n",
       "              167342,\n",
       "              46688,\n",
       "              104926,\n",
       "              90305,\n",
       "              12860,\n",
       "              209913,\n",
       "              14835,\n",
       "              30582,\n",
       "              232694,\n",
       "              163877,\n",
       "              193669,\n",
       "              111704,\n",
       "              160515,\n",
       "              163877,\n",
       "              111810,\n",
       "              92418,\n",
       "              150093,\n",
       "              172197,\n",
       "              11966,\n",
       "              123383,\n",
       "              21506,\n",
       "              190032,\n",
       "              163877,\n",
       "              236156,\n",
       "              80190,\n",
       "              48664,\n",
       "              25620,\n",
       "              1987,\n",
       "              150093,\n",
       "              93299,\n",
       "              70415,\n",
       "              112055,\n",
       "              18070,\n",
       "              77597,\n",
       "              163877,\n",
       "              143198,\n",
       "              200647,\n",
       "              210711,\n",
       "              105479,\n",
       "              148117,\n",
       "              163877,\n",
       "              208739,\n",
       "              83582,\n",
       "              93299,\n",
       "              138670,\n",
       "              201433,\n",
       "              119297,\n",
       "              225505,\n",
       "              34774,\n",
       "              106580,\n",
       "              25620,\n",
       "              168113,\n",
       "              182831,\n",
       "              43061,\n",
       "              163877,\n",
       "              56380,\n",
       "              148568,\n",
       "              229004,\n",
       "              78833,\n",
       "              205632,\n",
       "              163877,\n",
       "              104926,\n",
       "              40,\n",
       "              14958,\n",
       "              173034,\n",
       "              146957,\n",
       "              172797,\n",
       "              116662,\n",
       "              210311,\n",
       "              31738,\n",
       "              205298,\n",
       "              172797,\n",
       "              229004,\n",
       "              158216,\n",
       "              120353,\n",
       "              160106,\n",
       "              94604,\n",
       "              26907,\n",
       "              163877,\n",
       "              138677,\n",
       "              221404,\n",
       "              107980,\n",
       "              79126,\n",
       "              172197,\n",
       "              232250,\n",
       "              100664,\n",
       "              78630,\n",
       "              104926,\n",
       "              52453,\n",
       "              243585,\n",
       "              139219,\n",
       "              180572,\n",
       "              93299,\n",
       "              43783,\n",
       "              163877,\n",
       "              193669,\n",
       "              150492,\n",
       "              56102,\n",
       "              193669,\n",
       "              189083,\n",
       "              193669,\n",
       "              93299,\n",
       "              155019,\n",
       "              181285,\n",
       "              12638,\n",
       "              154435,\n",
       "              193669,\n",
       "              162416,\n",
       "              104926,\n",
       "              193669,\n",
       "              222686,\n",
       "              151372,\n",
       "              163877,\n",
       "              6,\n",
       "              193669,\n",
       "              163877,\n",
       "              163877,\n",
       "              151372,\n",
       "              198775,\n",
       "              222686,\n",
       "              86436,\n",
       "              193669,\n",
       "              54000,\n",
       "              6,\n",
       "              138677,\n",
       "              223783,\n",
       "              37609,\n",
       "              93299,\n",
       "              25620,\n",
       "              172730,\n",
       "              196703,\n",
       "              105479,\n",
       "              157631,\n",
       "              163877,\n",
       "              7173,\n",
       "              120402,\n",
       "              123383,\n",
       "              3646,\n",
       "              51608,\n",
       "              172757,\n",
       "              193669,\n",
       "              49025,\n",
       "              3646,\n",
       "              123383,\n",
       "              160515,\n",
       "              139798,\n",
       "              165676,\n",
       "              40843,\n",
       "              93299,\n",
       "              178439,\n",
       "              196832,\n",
       "              149306,\n",
       "              93299,\n",
       "              31738,\n",
       "              160515,\n",
       "              104926,\n",
       "              211201,\n",
       "              31738,\n",
       "              220725,\n",
       "              145605,\n",
       "              113836,\n",
       "              234362,\n",
       "              31738,\n",
       "              93299,\n",
       "              144889,\n",
       "              9894,\n",
       "              70014,\n",
       "              69055,\n",
       "              131234,\n",
       "              220037,\n",
       "              2521,\n",
       "              232694,\n",
       "              94604,\n",
       "              93299,\n",
       "              120100,\n",
       "              223503,\n",
       "              25620,\n",
       "              163877,\n",
       "              106770,\n",
       "              14031,\n",
       "              130104,\n",
       "              243585,\n",
       "              173974,\n",
       "              163877,\n",
       "              120817,\n",
       "              18070,\n",
       "              107613,\n",
       "              111963,\n",
       "              73710,\n",
       "              209910,\n",
       "              172797,\n",
       "              190032,\n",
       "              152974,\n",
       "              7173,\n",
       "              31109,\n",
       "              152792,\n",
       "              106580,\n",
       "              88364,\n",
       "              47991,\n",
       "              190032,\n",
       "              25620,\n",
       "              193669,\n",
       "              101099,\n",
       "              25697,\n",
       "              228024,\n",
       "              163877,\n",
       "              163877,\n",
       "              106580,\n",
       "              160515,\n",
       "              104926,\n",
       "              64146,\n",
       "              176160,\n",
       "              205377,\n",
       "              93299,\n",
       "              91454,\n",
       "              128300,\n",
       "              178236,\n",
       "              135800,\n",
       "              43061,\n",
       "              172730,\n",
       "              218302,\n",
       "              90305,\n",
       "              107980,\n",
       "              174862,\n",
       "              164808,\n",
       "              189083,\n",
       "              163877,\n",
       "              101896,\n",
       "              93299,\n",
       "              104926,\n",
       "              13900,\n",
       "              56095,\n",
       "              106580,\n",
       "              200313,\n",
       "              49871,\n",
       "              11735,\n",
       "              220725,\n",
       "              163877,\n",
       "              104926,\n",
       "              105479,\n",
       "              112055,\n",
       "              2026,\n",
       "              94619,\n",
       "              112055,\n",
       "              190032,\n",
       "              47069,\n",
       "              156816,\n",
       "              51032,\n",
       "              56353,\n",
       "              7173,\n",
       "              138920,\n",
       "              176445,\n",
       "              236046,\n",
       "              126787,\n",
       "              223503,\n",
       "              25620,\n",
       "              93299,\n",
       "              123383,\n",
       "              175304,\n",
       "              127823,\n",
       "              207397,\n",
       "              220725,\n",
       "              193669,\n",
       "              107613,\n",
       "              127856,\n",
       "              161828,\n",
       "              93299,\n",
       "              38477,\n",
       "              138503,\n",
       "              233203,\n",
       "              163877,\n",
       "              31738,\n",
       "              172197,\n",
       "              182828,\n",
       "              232694,\n",
       "              38866,\n",
       "              93299,\n",
       "              221220,\n",
       "              223503,\n",
       "              146225,\n",
       "              243133,\n",
       "              15493,\n",
       "              123383,\n",
       "              48951,\n",
       "              190032,\n",
       "              172197,\n",
       "              135409,\n",
       "              163877,\n",
       "              163877,\n",
       "              111704,\n",
       "              172197,\n",
       "              193669,\n",
       "              230380,\n",
       "              106580,\n",
       "              99640,\n",
       "              163877,\n",
       "              186696,\n",
       "              193669,\n",
       "              223273,\n",
       "              13595,\n",
       "              103758,\n",
       "              181285,\n",
       "              25897,\n",
       "              31159,\n",
       "              220725,\n",
       "              220725,\n",
       "              111727,\n",
       "              218090,\n",
       "              184949,\n",
       "              172197,\n",
       "              52262,\n",
       "              163877,\n",
       "              122015,\n",
       "              189689,\n",
       "              193669,\n",
       "              193669,\n",
       "              200934,\n",
       "              126787,\n",
       "              164450,\n",
       "              206643,\n",
       "              243585,\n",
       "              20110,\n",
       "              149637,\n",
       "              62620,\n",
       "              95389,\n",
       "              179781,\n",
       "              122112,\n",
       "              172197,\n",
       "              157972,\n",
       "              163877,\n",
       "              39895,\n",
       "              61111,\n",
       "              172197,\n",
       "              93299,\n",
       "              106059,\n",
       "              155799,\n",
       "              214166,\n",
       "              128725,\n",
       "              86956,\n",
       "              4506,\n",
       "              193669,\n",
       "              36227,\n",
       "              167909,\n",
       "              138258,\n",
       "              171376,\n",
       "              193669,\n",
       "              158983,\n",
       "              191970,\n",
       "              31738,\n",
       "              197129,\n",
       "              193669,\n",
       "              92184,\n",
       "              172197,\n",
       "              225644,\n",
       "              193669],\n",
       "             7: [48990,\n",
       "              93299,\n",
       "              158216,\n",
       "              154699,\n",
       "              35145,\n",
       "              218490,\n",
       "              72866,\n",
       "              193669,\n",
       "              156688,\n",
       "              97759,\n",
       "              65308,\n",
       "              93299,\n",
       "              48990,\n",
       "              243585,\n",
       "              180022,\n",
       "              145605,\n",
       "              220725,\n",
       "              145605,\n",
       "              145605,\n",
       "              220725,\n",
       "              65308,\n",
       "              93299,\n",
       "              48990,\n",
       "              154699,\n",
       "              14958,\n",
       "              220986,\n",
       "              112293,\n",
       "              65308,\n",
       "              81347,\n",
       "              220986,\n",
       "              131330,\n",
       "              79388,\n",
       "              174508,\n",
       "              48990,\n",
       "              15167,\n",
       "              93299,\n",
       "              102329,\n",
       "              229880,\n",
       "              193669,\n",
       "              93299,\n",
       "              228087,\n",
       "              135176,\n",
       "              163877,\n",
       "              65308,\n",
       "              252665,\n",
       "              48990,\n",
       "              7173,\n",
       "              175248,\n",
       "              183193,\n",
       "              135176,\n",
       "              79797,\n",
       "              157361,\n",
       "              193669,\n",
       "              48990,\n",
       "              18070,\n",
       "              69093,\n",
       "              222557,\n",
       "              79797,\n",
       "              232903,\n",
       "              157361,\n",
       "              135176,\n",
       "              200313,\n",
       "              193669,\n",
       "              124842,\n",
       "              101099,\n",
       "              93299,\n",
       "              154699,\n",
       "              154699,\n",
       "              138827,\n",
       "              83135,\n",
       "              93353,\n",
       "              200313,\n",
       "              159909,\n",
       "              31993,\n",
       "              200313,\n",
       "              157664,\n",
       "              154699,\n",
       "              112055,\n",
       "              218490,\n",
       "              154699,\n",
       "              128660,\n",
       "              48990,\n",
       "              81347,\n",
       "              200313,\n",
       "              200313,\n",
       "              148452,\n",
       "              93353,\n",
       "              35145,\n",
       "              185319,\n",
       "              180022,\n",
       "              200313,\n",
       "              48990,\n",
       "              212752,\n",
       "              200313,\n",
       "              154699,\n",
       "              148452,\n",
       "              218490,\n",
       "              35145,\n",
       "              167607,\n",
       "              180022,\n",
       "              69093,\n",
       "              48990,\n",
       "              179853,\n",
       "              93299,\n",
       "              64899,\n",
       "              229880,\n",
       "              163877,\n",
       "              163877,\n",
       "              187082,\n",
       "              35145,\n",
       "              81347,\n",
       "              48990,\n",
       "              105957,\n",
       "              172197,\n",
       "              72183,\n",
       "              243585,\n",
       "              79797,\n",
       "              139219,\n",
       "              7173,\n",
       "              139219,\n",
       "              214253,\n",
       "              48990,\n",
       "              66972,\n",
       "              200313,\n",
       "              87157,\n",
       "              129461,\n",
       "              13595,\n",
       "              35145,\n",
       "              9718,\n",
       "              163877,\n",
       "              154699,\n",
       "              65308,\n",
       "              246110,\n",
       "              48990,\n",
       "              213550,\n",
       "              180022,\n",
       "              145605,\n",
       "              117084,\n",
       "              190353,\n",
       "              4548,\n",
       "              145605,\n",
       "              65308,\n",
       "              138844,\n",
       "              48990,\n",
       "              177103,\n",
       "              220725,\n",
       "              183836,\n",
       "              145605,\n",
       "              209297,\n",
       "              145605,\n",
       "              190353,\n",
       "              48990,\n",
       "              138844,\n",
       "              243585,\n",
       "              8241,\n",
       "              139219,\n",
       "              241835,\n",
       "              139219,\n",
       "              181264,\n",
       "              139219,\n",
       "              178472,\n",
       "              48990,\n",
       "              138844,\n",
       "              243585,\n",
       "              240742,\n",
       "              129461,\n",
       "              163877,\n",
       "              93934,\n",
       "              48990,\n",
       "              7173,\n",
       "              106580,\n",
       "              181285,\n",
       "              48736,\n",
       "              79797,\n",
       "              172790,\n",
       "              184472,\n",
       "              53367,\n",
       "              162602,\n",
       "              133525,\n",
       "              51646,\n",
       "              105129,\n",
       "              48990,\n",
       "              252665,\n",
       "              142913,\n",
       "              18383,\n",
       "              25620,\n",
       "              201891,\n",
       "              241362,\n",
       "              18070,\n",
       "              65308,\n",
       "              93353,\n",
       "              48990,\n",
       "              185319,\n",
       "              180022,\n",
       "              200313,\n",
       "              154699,\n",
       "              161828,\n",
       "              93299,\n",
       "              35897,\n",
       "              181459,\n",
       "              12171,\n",
       "              138677,\n",
       "              193669,\n",
       "              229004,\n",
       "              220986,\n",
       "              220986,\n",
       "              131330,\n",
       "              48990,\n",
       "              183444,\n",
       "              93299,\n",
       "              224211,\n",
       "              112293,\n",
       "              48990,\n",
       "              32453,\n",
       "              214415,\n",
       "              232903,\n",
       "              93353,\n",
       "              128106,\n",
       "              93299,\n",
       "              109868,\n",
       "              14958,\n",
       "              100824,\n",
       "              48990,\n",
       "              112055,\n",
       "              7681,\n",
       "              25620,\n",
       "              179790,\n",
       "              48990,\n",
       "              252665,\n",
       "              200313,\n",
       "              223077,\n",
       "              247832,\n",
       "              46688,\n",
       "              193669,\n",
       "              85359,\n",
       "              163877,\n",
       "              57367,\n",
       "              48990,\n",
       "              169695,\n",
       "              163877,\n",
       "              166398,\n",
       "              65222,\n",
       "              192674,\n",
       "              93299,\n",
       "              120942,\n",
       "              114386,\n",
       "              48990,\n",
       "              29201,\n",
       "              130678,\n",
       "              55315,\n",
       "              169397,\n",
       "              154699,\n",
       "              93299,\n",
       "              93771,\n",
       "              48990,\n",
       "              181631,\n",
       "              52067,\n",
       "              52067,\n",
       "              80713,\n",
       "              11975,\n",
       "              78833,\n",
       "              217432,\n",
       "              7274,\n",
       "              90218,\n",
       "              48990,\n",
       "              93299,\n",
       "              221102,\n",
       "              244226,\n",
       "              145187,\n",
       "              18070,\n",
       "              116288,\n",
       "              70415,\n",
       "              70082,\n",
       "              220986],\n",
       "             8: [171558,\n",
       "              153537,\n",
       "              83829,\n",
       "              124647,\n",
       "              220477,\n",
       "              145605,\n",
       "              42681,\n",
       "              145605,\n",
       "              153537,\n",
       "              124647,\n",
       "              138677,\n",
       "              250086,\n",
       "              170823,\n",
       "              93299,\n",
       "              153537,\n",
       "              124647,\n",
       "              124647,\n",
       "              129746,\n",
       "              203721,\n",
       "              134454,\n",
       "              210454,\n",
       "              31738,\n",
       "              93299,\n",
       "              111981,\n",
       "              236046,\n",
       "              240206,\n",
       "              117670,\n",
       "              180327,\n",
       "              107613,\n",
       "              86521,\n",
       "              135176,\n",
       "              199004,\n",
       "              119297,\n",
       "              153537,\n",
       "              42681,\n",
       "              124647,\n",
       "              220725,\n",
       "              7011,\n",
       "              145605,\n",
       "              193669,\n",
       "              139219,\n",
       "              135778,\n",
       "              161828,\n",
       "              7960,\n",
       "              123383,\n",
       "              211131,\n",
       "              211735,\n",
       "              172197,\n",
       "              219274,\n",
       "              117951,\n",
       "              128401,\n",
       "              93299,\n",
       "              4344,\n",
       "              106580,\n",
       "              163877,\n",
       "              65845,\n",
       "              65910,\n",
       "              19819,\n",
       "              198788,\n",
       "              123383,\n",
       "              193669,\n",
       "              31738,\n",
       "              25620,\n",
       "              194994,\n",
       "              19819,\n",
       "              8,\n",
       "              230685,\n",
       "              200313,\n",
       "              129984,\n",
       "              65845,\n",
       "              219881,\n",
       "              200313,\n",
       "              194994,\n",
       "              65845,\n",
       "              31738,\n",
       "              8,\n",
       "              25620,\n",
       "              19819,\n",
       "              44933,\n",
       "              169435,\n",
       "              198067,\n",
       "              193669,\n",
       "              110286,\n",
       "              151953,\n",
       "              147603,\n",
       "              18669,\n",
       "              172197,\n",
       "              198067,\n",
       "              161828,\n",
       "              52262,\n",
       "              178472,\n",
       "              39133,\n",
       "              239577,\n",
       "              163877,\n",
       "              91453,\n",
       "              52262,\n",
       "              106580,\n",
       "              193669,\n",
       "              163877,\n",
       "              77779,\n",
       "              51232,\n",
       "              139219,\n",
       "              193669,\n",
       "              201277,\n",
       "              141067,\n",
       "              145605,\n",
       "              200313,\n",
       "              172197,\n",
       "              3569,\n",
       "              1436,\n",
       "              135176,\n",
       "              89962,\n",
       "              119077,\n",
       "              240452,\n",
       "              12433,\n",
       "              221594,\n",
       "              78138,\n",
       "              7246,\n",
       "              12638,\n",
       "              104612,\n",
       "              12433,\n",
       "              221594,\n",
       "              163877,\n",
       "              220725,\n",
       "              89962,\n",
       "              161581,\n",
       "              161581,\n",
       "              89962,\n",
       "              243585,\n",
       "              163877,\n",
       "              153537,\n",
       "              124647,\n",
       "              194407,\n",
       "              163877,\n",
       "              70942,\n",
       "              112055,\n",
       "              153537,\n",
       "              6844,\n",
       "              124647,\n",
       "              194407,\n",
       "              163877,\n",
       "              42681,\n",
       "              200313,\n",
       "              240417,\n",
       "              248728,\n",
       "              89962,\n",
       "              194407,\n",
       "              148012,\n",
       "              65845,\n",
       "              131983,\n",
       "              163877,\n",
       "              55433,\n",
       "              18470,\n",
       "              153537,\n",
       "              157866,\n",
       "              124647,\n",
       "              163877,\n",
       "              220725,\n",
       "              135765,\n",
       "              139219,\n",
       "              242091,\n",
       "              139219,\n",
       "              138626,\n",
       "              111981,\n",
       "              209840,\n",
       "              31738,\n",
       "              31738,\n",
       "              209840,\n",
       "              209840,\n",
       "              193669,\n",
       "              160515,\n",
       "              191656,\n",
       "              147076,\n",
       "              153537,\n",
       "              22003,\n",
       "              124647,\n",
       "              124647,\n",
       "              18070,\n",
       "              127518,\n",
       "              70415,\n",
       "              235005,\n",
       "              240273,\n",
       "              140800,\n",
       "              153537,\n",
       "              131434,\n",
       "              124647,\n",
       "              102772,\n",
       "              31738,\n",
       "              5195,\n",
       "              89962,\n",
       "              144021,\n",
       "              78138,\n",
       "              140800,\n",
       "              153537,\n",
       "              233467,\n",
       "              124647,\n",
       "              13595,\n",
       "              31738,\n",
       "              162860,\n",
       "              89962,\n",
       "              163877,\n",
       "              220337,\n",
       "              171558,\n",
       "              153537,\n",
       "              83829,\n",
       "              124647,\n",
       "              220725,\n",
       "              139219,\n",
       "              145605,\n",
       "              79797,\n",
       "              153537,\n",
       "              56102,\n",
       "              124647,\n",
       "              135176,\n",
       "              93299,\n",
       "              244864,\n",
       "              158036,\n",
       "              238377,\n",
       "              124647],\n",
       "             9: [200313,\n",
       "              200313,\n",
       "              102565,\n",
       "              169709,\n",
       "              91510,\n",
       "              102661,\n",
       "              115967,\n",
       "              204236,\n",
       "              129863,\n",
       "              55663,\n",
       "              129863,\n",
       "              230234,\n",
       "              111955,\n",
       "              200313,\n",
       "              172652,\n",
       "              126414,\n",
       "              205106,\n",
       "              173314,\n",
       "              129863,\n",
       "              98578,\n",
       "              111955,\n",
       "              110260,\n",
       "              172652,\n",
       "              200313,\n",
       "              193669,\n",
       "              25144,\n",
       "              2252,\n",
       "              96679,\n",
       "              129863,\n",
       "              93353,\n",
       "              111955,\n",
       "              184293,\n",
       "              172652,\n",
       "              15264,\n",
       "              193669,\n",
       "              34185,\n",
       "              193669,\n",
       "              129863,\n",
       "              93353,\n",
       "              111955,\n",
       "              247768,\n",
       "              172652,\n",
       "              3084,\n",
       "              193669,\n",
       "              184176,\n",
       "              119632,\n",
       "              181264,\n",
       "              155941,\n",
       "              23967,\n",
       "              200313,\n",
       "              93245,\n",
       "              4619,\n",
       "              200313,\n",
       "              164575,\n",
       "              28658,\n",
       "              3646,\n",
       "              56053,\n",
       "              243585,\n",
       "              120763,\n",
       "              1361,\n",
       "              190353,\n",
       "              243585,\n",
       "              220725,\n",
       "              190353,\n",
       "              42681],\n",
       "             10: [194418,\n",
       "              112472,\n",
       "              20567,\n",
       "              194418,\n",
       "              83599,\n",
       "              25620,\n",
       "              194418,\n",
       "              204003,\n",
       "              112472,\n",
       "              200313],\n",
       "             11: [167607,\n",
       "              123383,\n",
       "              185028,\n",
       "              106580,\n",
       "              107613,\n",
       "              224023,\n",
       "              161828,\n",
       "              25620,\n",
       "              25897,\n",
       "              215903,\n",
       "              160515,\n",
       "              203466,\n",
       "              200313,\n",
       "              80021,\n",
       "              164808,\n",
       "              90916,\n",
       "              10464,\n",
       "              123383,\n",
       "              38197,\n",
       "              119989,\n",
       "              123383,\n",
       "              12638,\n",
       "              93299,\n",
       "              25897,\n",
       "              123383,\n",
       "              74323,\n",
       "              164808,\n",
       "              93299,\n",
       "              223405,\n",
       "              65222,\n",
       "              106580,\n",
       "              224211,\n",
       "              53361,\n",
       "              112055,\n",
       "              123383,\n",
       "              116288,\n",
       "              119989,\n",
       "              11735,\n",
       "              93299,\n",
       "              106580,\n",
       "              176097,\n",
       "              123383,\n",
       "              22171,\n",
       "              123383,\n",
       "              196514,\n",
       "              93299,\n",
       "              85494,\n",
       "              156958,\n",
       "              25897,\n",
       "              12638,\n",
       "              57711,\n",
       "              167607,\n",
       "              38389,\n",
       "              123383,\n",
       "              10863,\n",
       "              112055,\n",
       "              144885,\n",
       "              64129,\n",
       "              93299,\n",
       "              15912,\n",
       "              139414,\n",
       "              206174,\n",
       "              15912,\n",
       "              149221,\n",
       "              57202,\n",
       "              106580,\n",
       "              167607,\n",
       "              25897,\n",
       "              123383,\n",
       "              54767,\n",
       "              164808,\n",
       "              74323,\n",
       "              149637,\n",
       "              107613],\n",
       "             12: [106580,\n",
       "              172851,\n",
       "              209297,\n",
       "              10464,\n",
       "              246958,\n",
       "              219576,\n",
       "              207529,\n",
       "              24107,\n",
       "              94604,\n",
       "              185028,\n",
       "              207301,\n",
       "              172851,\n",
       "              93299,\n",
       "              82957,\n",
       "              96341,\n",
       "              25897,\n",
       "              121240,\n",
       "              163877,\n",
       "              243585,\n",
       "              93299,\n",
       "              232903,\n",
       "              235633,\n",
       "              180413,\n",
       "              172197,\n",
       "              243936,\n",
       "              14958,\n",
       "              203466,\n",
       "              250538,\n",
       "              163877,\n",
       "              249746,\n",
       "              50473,\n",
       "              229019,\n",
       "              37908,\n",
       "              174100,\n",
       "              52598,\n",
       "              113836,\n",
       "              103524,\n",
       "              31738,\n",
       "              63688,\n",
       "              93299,\n",
       "              135158,\n",
       "              145187,\n",
       "              200934,\n",
       "              193669,\n",
       "              7173,\n",
       "              220732,\n",
       "              241362,\n",
       "              52262,\n",
       "              163877,\n",
       "              103482,\n",
       "              161828,\n",
       "              199424,\n",
       "              3798,\n",
       "              175248,\n",
       "              152742,\n",
       "              117710,\n",
       "              123383,\n",
       "              250222,\n",
       "              107613,\n",
       "              128114,\n",
       "              163877,\n",
       "              46931,\n",
       "              93299,\n",
       "              21506,\n",
       "              5519,\n",
       "              104607,\n",
       "              90305,\n",
       "              149626,\n",
       "              172797,\n",
       "              7173,\n",
       "              236046,\n",
       "              111269,\n",
       "              18070,\n",
       "              123383,\n",
       "              18070,\n",
       "              105129,\n",
       "              123383,\n",
       "              149637,\n",
       "              204930,\n",
       "              183004,\n",
       "              198067,\n",
       "              14567,\n",
       "              164450,\n",
       "              163877,\n",
       "              203721,\n",
       "              178468,\n",
       "              66520,\n",
       "              163877,\n",
       "              85297,\n",
       "              231637,\n",
       "              93190,\n",
       "              133956,\n",
       "              7173,\n",
       "              219697,\n",
       "              59316,\n",
       "              46722,\n",
       "              123383,\n",
       "              75886,\n",
       "              137562,\n",
       "              190353,\n",
       "              193669,\n",
       "              190353,\n",
       "              95389,\n",
       "              94604,\n",
       "              142650,\n",
       "              231637,\n",
       "              150820,\n",
       "              222229,\n",
       "              248728,\n",
       "              105129,\n",
       "              193669,\n",
       "              93299,\n",
       "              231637,\n",
       "              162043,\n",
       "              124306,\n",
       "              163877,\n",
       "              107613,\n",
       "              193669,\n",
       "              186328,\n",
       "              28376,\n",
       "              112055,\n",
       "              93299,\n",
       "              190032,\n",
       "              142650,\n",
       "              173538,\n",
       "              156189,\n",
       "              112055,\n",
       "              185298,\n",
       "              29809,\n",
       "              14958,\n",
       "              236046,\n",
       "              93299,\n",
       "              107613,\n",
       "              198067,\n",
       "              107613,\n",
       "              9433,\n",
       "              18070,\n",
       "              171905,\n",
       "              88140,\n",
       "              241362,\n",
       "              50997,\n",
       "              93299,\n",
       "              123383,\n",
       "              35897,\n",
       "              192562,\n",
       "              93299,\n",
       "              79797,\n",
       "              16587,\n",
       "              93299,\n",
       "              93299,\n",
       "              55095,\n",
       "              163877,\n",
       "              52262,\n",
       "              7173,\n",
       "              50473,\n",
       "              52453,\n",
       "              25620,\n",
       "              220725,\n",
       "              172329,\n",
       "              139219,\n",
       "              247206,\n",
       "              139219,\n",
       "              201277,\n",
       "              163877,\n",
       "              60947,\n",
       "              206036,\n",
       "              182794,\n",
       "              53367,\n",
       "              25897,\n",
       "              193669,\n",
       "              219176,\n",
       "              114316,\n",
       "              236672,\n",
       "              7173,\n",
       "              132942,\n",
       "              79797,\n",
       "              193669,\n",
       "              148871,\n",
       "              200934,\n",
       "              172197,\n",
       "              148478,\n",
       "              163877,\n",
       "              93299,\n",
       "              186328,\n",
       "              240833,\n",
       "              211934,\n",
       "              52598,\n",
       "              123395,\n",
       "              157338,\n",
       "              93299,\n",
       "              163877,\n",
       "              211605,\n",
       "              135176,\n",
       "              141513,\n",
       "              107613,\n",
       "              177870,\n",
       "              185028,\n",
       "              163877,\n",
       "              153375,\n",
       "              22467,\n",
       "              79797,\n",
       "              111269,\n",
       "              244864,\n",
       "              93299,\n",
       "              88507,\n",
       "              163877,\n",
       "              193669,\n",
       "              94426,\n",
       "              172540,\n",
       "              175248,\n",
       "              24107,\n",
       "              119589,\n",
       "              93299,\n",
       "              163877,\n",
       "              201032,\n",
       "              238927,\n",
       "              14567,\n",
       "              209297,\n",
       "              7173,\n",
       "              10305,\n",
       "              14420,\n",
       "              5376,\n",
       "              133421,\n",
       "              79797,\n",
       "              6651,\n",
       "              230685,\n",
       "              163505,\n",
       "              105129,\n",
       "              105129,\n",
       "              13819,\n",
       "              12,\n",
       "              216653,\n",
       "              106580,\n",
       "              111614,\n",
       "              198067,\n",
       "              105129,\n",
       "              106580,\n",
       "              105129,\n",
       "              198067,\n",
       "              163505,\n",
       "              236880,\n",
       "              12,\n",
       "              235975,\n",
       "              105129,\n",
       "              105129,\n",
       "              13819,\n",
       "              106580,\n",
       "              105129,\n",
       "              123383,\n",
       "              235975,\n",
       "              185028,\n",
       "              236880,\n",
       "              198067,\n",
       "              106580,\n",
       "              172197,\n",
       "              107613,\n",
       "              123383,\n",
       "              210022,\n",
       "              183611,\n",
       "              25620,\n",
       "              56292,\n",
       "              36522,\n",
       "              31738,\n",
       "              94604,\n",
       "              172197,\n",
       "              93299,\n",
       "              170969,\n",
       "              39133,\n",
       "              131055,\n",
       "              163877,\n",
       "              172197,\n",
       "              142650,\n",
       "              163877,\n",
       "              2521,\n",
       "              110931,\n",
       "              110931,\n",
       "              161828,\n",
       "              123383,\n",
       "              211757,\n",
       "              94604,\n",
       "              121464,\n",
       "              52598,\n",
       "              193208,\n",
       "              185028,\n",
       "              198067,\n",
       "              111269,\n",
       "              171558,\n",
       "              175817,\n",
       "              181187,\n",
       "              25620,\n",
       "              172197,\n",
       "              96338,\n",
       "              203692,\n",
       "              79797,\n",
       "              123383,\n",
       "              93299,\n",
       "              117710,\n",
       "              117710,\n",
       "              79797,\n",
       "              123383,\n",
       "              123383,\n",
       "              107613,\n",
       "              28923,\n",
       "              172197,\n",
       "              193669,\n",
       "              18070,\n",
       "              157628,\n",
       "              252894,\n",
       "              158216,\n",
       "              88603,\n",
       "              182828,\n",
       "              105129,\n",
       "              123383,\n",
       "              169960,\n",
       "              20525,\n",
       "              61611,\n",
       "              75436,\n",
       "              86936,\n",
       "              164808,\n",
       "              244864,\n",
       "              112055,\n",
       "              182513,\n",
       "              163877,\n",
       "              215486,\n",
       "              175304,\n",
       "              181549,\n",
       "              234501,\n",
       "              103784,\n",
       "              193669,\n",
       "              213300,\n",
       "              25897,\n",
       "              109868,\n",
       "              163877,\n",
       "              229428,\n",
       "              213709,\n",
       "              172197,\n",
       "              119946,\n",
       "              78841,\n",
       "              52262,\n",
       "              8722,\n",
       "              42681,\n",
       "              1516,\n",
       "              246958,\n",
       "              50473,\n",
       "              123383,\n",
       "              25620,\n",
       "              119705,\n",
       "              175841,\n",
       "              112055,\n",
       "              201708,\n",
       "              138502,\n",
       "              172197,\n",
       "              93299,\n",
       "              113119,\n",
       "              28507,\n",
       "              25620,\n",
       "              116913,\n",
       "              182854,\n",
       "              79797,\n",
       "              135828,\n",
       "              107613,\n",
       "              172197,\n",
       "              55125,\n",
       "              163877,\n",
       "              149908,\n",
       "              164085,\n",
       "              93299,\n",
       "              109868,\n",
       "              94776,\n",
       "              203721,\n",
       "              52598,\n",
       "              155387,\n",
       "              231637,\n",
       "              38477,\n",
       "              10464,\n",
       "              142650,\n",
       "              157631,\n",
       "              112293,\n",
       "              112055,\n",
       "              61611,\n",
       "              105408,\n",
       "              133525,\n",
       "              61044,\n",
       "              112055,\n",
       "              105129,\n",
       "              28923,\n",
       "              60640,\n",
       "              104612,\n",
       "              172197,\n",
       "              117670,\n",
       "              47393,\n",
       "              185028,\n",
       "              119705,\n",
       "              12171,\n",
       "              93299,\n",
       "              7173,\n",
       "              50473,\n",
       "              239577,\n",
       "              25620,\n",
       "              93008,\n",
       "              238613,\n",
       "              206643,\n",
       "              83372,\n",
       "              43061,\n",
       "              106580,\n",
       "              220725,\n",
       "              129875,\n",
       "              145605,\n",
       "              163877,\n",
       "              201277,\n",
       "              185028,\n",
       "              201277,\n",
       "              190353,\n",
       "              163877,\n",
       "              3646,\n",
       "              78833,\n",
       "              105129,\n",
       "              201032,\n",
       "              231637,\n",
       "              33112,\n",
       "              1516,\n",
       "              161828,\n",
       "              187187,\n",
       "              123292,\n",
       "              15776,\n",
       "              105129,\n",
       "              94604,\n",
       "              241169,\n",
       "              104612,\n",
       "              169834,\n",
       "              7173,\n",
       "              50473,\n",
       "              93299,\n",
       "              25620,\n",
       "              163877,\n",
       "              150820,\n",
       "              178624,\n",
       "              243585,\n",
       "              138677,\n",
       "              163877,\n",
       "              172197,\n",
       "              107613,\n",
       "              224211,\n",
       "              137831,\n",
       "              25620,\n",
       "              101099,\n",
       "              233467,\n",
       "              159729,\n",
       "              7173,\n",
       "              78138,\n",
       "              39785,\n",
       "              31738,\n",
       "              31738,\n",
       "              12069,\n",
       "              119705,\n",
       "              154285,\n",
       "              93299,\n",
       "              163877,\n",
       "              112055,\n",
       "              180671,\n",
       "              123383,\n",
       "              107613,\n",
       "              158357,\n",
       "              178312,\n",
       "              82275,\n",
       "              31738,\n",
       "              87301,\n",
       "              7173,\n",
       "              142650,\n",
       "              79797,\n",
       "              116662,\n",
       "              237216,\n",
       "              133127,\n",
       "              163877,\n",
       "              226098,\n",
       "              48822,\n",
       "              79797],\n",
       "             13: [146176,\n",
       "              1561,\n",
       "              83829,\n",
       "              243585,\n",
       "              220725,\n",
       "              3646,\n",
       "              145605,\n",
       "              145605,\n",
       "              3646,\n",
       "              220725,\n",
       "              120394,\n",
       "              1561,\n",
       "              163877,\n",
       "              247159,\n",
       "              173538,\n",
       "              210781,\n",
       "              70299,\n",
       "              172197,\n",
       "              65845,\n",
       "              209297],\n",
       "             14: [162932,\n",
       "              34577,\n",
       "              239846,\n",
       "              163877,\n",
       "              162932,\n",
       "              2521,\n",
       "              168571,\n",
       "              193669,\n",
       "              160515,\n",
       "              21892,\n",
       "              206514,\n",
       "              201277,\n",
       "              171558,\n",
       "              145605,\n",
       "              122252,\n",
       "              145605,\n",
       "              220477,\n",
       "              220725,\n",
       "              220725,\n",
       "              201277,\n",
       "              206514,\n",
       "              201277,\n",
       "              171558,\n",
       "              122252,\n",
       "              181264,\n",
       "              220725],\n",
       "             15: [102271,\n",
       "              14958,\n",
       "              235470,\n",
       "              213092,\n",
       "              158216,\n",
       "              160515,\n",
       "              214253,\n",
       "              47238,\n",
       "              236046,\n",
       "              104653,\n",
       "              189201,\n",
       "              200121,\n",
       "              34113,\n",
       "              96140,\n",
       "              164892,\n",
       "              80232,\n",
       "              157255,\n",
       "              207549,\n",
       "              95255,\n",
       "              196703,\n",
       "              106580,\n",
       "              93299],\n",
       "             16: [150726,\n",
       "              169712,\n",
       "              217726,\n",
       "              243585,\n",
       "              180310,\n",
       "              158977,\n",
       "              219861,\n",
       "              248296,\n",
       "              32494,\n",
       "              92444],\n",
       "             17: [220725,\n",
       "              139219,\n",
       "              3646,\n",
       "              209297,\n",
       "              1361,\n",
       "              128725,\n",
       "              1361,\n",
       "              163877,\n",
       "              209297,\n",
       "              95738,\n",
       "              139219,\n",
       "              70388,\n",
       "              139219,\n",
       "              220725,\n",
       "              133018,\n",
       "              31738,\n",
       "              138962,\n",
       "              161828,\n",
       "              107440,\n",
       "              49871,\n",
       "              133018,\n",
       "              237092,\n",
       "              154136,\n",
       "              175059,\n",
       "              193669,\n",
       "              200313,\n",
       "              268,\n",
       "              1361,\n",
       "              39268,\n",
       "              143582,\n",
       "              133018,\n",
       "              150255,\n",
       "              18171,\n",
       "              93299,\n",
       "              152943,\n",
       "              11198,\n",
       "              7173,\n",
       "              67733,\n",
       "              221957,\n",
       "              133018,\n",
       "              18070,\n",
       "              70415,\n",
       "              148040,\n",
       "              9735,\n",
       "              171558,\n",
       "              223007,\n",
       "              209155,\n",
       "              243585,\n",
       "              93299,\n",
       "              243585,\n",
       "              145605,\n",
       "              220725,\n",
       "              171558,\n",
       "              195504,\n",
       "              209297,\n",
       "              1361,\n",
       "              145605,\n",
       "              220725,\n",
       "              245470,\n",
       "              162860,\n",
       "              109432,\n",
       "              86436,\n",
       "              93006,\n",
       "              169302,\n",
       "              24928,\n",
       "              163877,\n",
       "              172197,\n",
       "              153508,\n",
       "              112055,\n",
       "              133018,\n",
       "              237930,\n",
       "              91453,\n",
       "              3646,\n",
       "              118222,\n",
       "              139219,\n",
       "              25620,\n",
       "              220725,\n",
       "              119668,\n",
       "              94081,\n",
       "              100793,\n",
       "              31738,\n",
       "              169244,\n",
       "              201299,\n",
       "              133910,\n",
       "              133018,\n",
       "              119668,\n",
       "              209297,\n",
       "              220477,\n",
       "              1361,\n",
       "              220725,\n",
       "              145605,\n",
       "              145605,\n",
       "              220725,\n",
       "              64140,\n",
       "              133018,\n",
       "              98807,\n",
       "              93299,\n",
       "              220725,\n",
       "              175059,\n",
       "              139219,\n",
       "              139219,\n",
       "              73177,\n",
       "              133018,\n",
       "              48825,\n",
       "              7173,\n",
       "              31738,\n",
       "              224882,\n",
       "              123383,\n",
       "              203466,\n",
       "              11198,\n",
       "              133018,\n",
       "              44943,\n",
       "              175059,\n",
       "              48254,\n",
       "              52988,\n",
       "              167452,\n",
       "              148040,\n",
       "              1361,\n",
       "              25620,\n",
       "              195504,\n",
       "              239992,\n",
       "              224690,\n",
       "              241362,\n",
       "              47300,\n",
       "              172012,\n",
       "              123383,\n",
       "              225355,\n",
       "              25620,\n",
       "              195504,\n",
       "              75109,\n",
       "              132351,\n",
       "              93299,\n",
       "              217361,\n",
       "              107613,\n",
       "              163877,\n",
       "              161828,\n",
       "              3159,\n",
       "              23642,\n",
       "              195504,\n",
       "              172197,\n",
       "              93299,\n",
       "              106355,\n",
       "              83372,\n",
       "              245470,\n",
       "              172197,\n",
       "              195504,\n",
       "              158216,\n",
       "              7173,\n",
       "              76171,\n",
       "              20502,\n",
       "              34409,\n",
       "              187516,\n",
       "              163877,\n",
       "              237925],\n",
       "             18: [105129,\n",
       "              227137,\n",
       "              123383,\n",
       "              194806,\n",
       "              206118,\n",
       "              215590,\n",
       "              93299,\n",
       "              49236],\n",
       "             19: [150732, 102150, 6761, 104964, 177091, 150732, 31139]})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dict = defaultdict(list)\n",
    "for i in range(20):\n",
    "    tmp_dict[i] = ds.pair_dict[i]\n",
    "tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "len_lists = []\n",
    "for x,y in ds.pair_dict.items():\n",
    "     len_lists.append(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean = 292.75764021839325\n",
      "median = 15.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'statistics' has no attribute 'min'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-93ef5e02e02d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"mean = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"median = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"minimum = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'statistics' has no attribute 'min'"
     ]
    }
   ],
   "source": [
    "print( \"mean = \" + str(statistics.mean(len_lists)))\n",
    "print( \"median = \" + str(statistics.median(len_lists)))\n",
    "print( \"minimum = \" + str(statistics.min(len_lists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "0%=====50%, Time:  05:10:53.55, cum_loss = 675026.375\n",
      "1 epoch of 20\n",
      " 225340 225341 batches, pairs 73938034, cum loss: 675030.25000\n",
      "Current score on wordsim Task: 0.5582553525247311\n",
      "batch_size = 28 || processed_batches = 92451/22534151\r"
     ]
    }
   ],
   "source": [
    "#w/o 'the'\n",
    "w2v = W2V(ds, neg_samples=10, alpha=0.007,shuffle=True)\n",
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'a'), (1, 'b'), (2, 'c'), (4, 'd'), (3, 'e'), (3, 'f'), (3, 'g'), (3, 'h'), (3, 'i'), (3, 'j'), (0, 10), (1, 9), (2, 8), (4, 7), (3, 6), (3, 5), (3, 4), (3, 3), (3, 2), (3, 1), (3, 0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['a', 10],\n",
       "             1: ['b', 9],\n",
       "             2: ['c', 8],\n",
       "             4: ['d', 7],\n",
       "             3: ['e', 'f', 'g', 'h', 'i', 'j', 6, 5, 4, 3, 2, 1, 0]})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "test_dict = defaultdict(list)\n",
    "alphabet = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\"]\n",
    "upwards = [0,1,2,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]\n",
    "downwards = [x for x in range(10,-1,-1)]\n",
    "l = list(zip(upwards,alphabet))\n",
    "l.extend(list(zip(upwards,downwards)))\n",
    "print(l)\n",
    "for x in l: \n",
    "    test_dict[x[0]].append(x[1])\n",
    "test_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(223034, 2582), (18919, 4890), (18070, 1121414), (123383, 2770426), (215113, 61170), (193669, 5043344), (68216, 4763), (203466, 245019), (65222, 192850), (204585, 71527), (177779, 86475), (51888, 19272), (133421, 28965), (189190, 992), (81347, 81766), (163877, 9018219), (122399, 212), (88603, 100927), (24556, 17165), (93299, 3542351)]\n",
      "9018219\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "def concatenate_dict(sampling_dict):\n",
    "    new_dict = defaultdict(list)\n",
    "    look_up_table = []\n",
    "    for x,y in sampling_dict.items():\n",
    "        look_up_table.append((x,len(y)))\n",
    "    max_value = max(look_up_table,key=operator.itemgetter(1))[1]\n",
    "    print(look_up_table[0:20])\n",
    "    print(max_value)\n",
    "    i =0 \n",
    "    new_keys = []\n",
    "    while (i < len(look_up_table)): \n",
    "        j = 1 \n",
    "        sum = look_up_table[i][1]\n",
    "        concatenations = [look_up_table[i][0]]\n",
    "        while (j+i < len(look_up_table) and (sum + (look_up_table[i+j][1])) < max_value and sum <2000):\n",
    "            #print(\" i = \" + str(i)+ \"j = \" + str(j))\n",
    "            sum += look_up_table[i+j][1]\n",
    "            concatenations.append(look_up_table[i+j][0])\n",
    "            j+=1\n",
    "        new_keys.append(concatenations)\n",
    "        i+=j\n",
    "\n",
    "    return (new_keys)\n",
    "        \n",
    "new_keys = concatenate_dict(ds.pair_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10388.4030472905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13914"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for x in new_keys:\n",
    "    y = sum([len(ds.pair_dict[z]) for z in x])\n",
    "    #print(y)\n",
    "    l.append(y)\n",
    "print(np.mean(l))\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13914"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vocab\n"
     ]
    }
   ],
   "source": [
    "# Creating whole datasety\n",
    "enwik9_wDataset = wDataSet((enwik9),ctx_window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "0%==========100%, Time:  02:57:43.88, cum_loss = 542836.375\n",
      "1 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 542851.43750\n",
      "Current score on wordsim Task: 0.6580580246735275\n",
      "0%==========100%, Time:  02:57:25.32, cum_loss = 524791.9375\n",
      "2 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 524807.06250\n",
      "Current score on wordsim Task: 0.6651399366885339\n",
      "0%==========100%, Time:  02:42:56.22, cum_loss = 523478.125\n",
      "3 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 523493.21875\n",
      "Current score on wordsim Task: 0.6644381396072672\n",
      "0%==========100%, Time:  02:58:48.60, cum_loss = 523088.78125\n",
      "4 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 523103.78125\n",
      "Current score on wordsim Task: 0.6716186405794663\n",
      "0%==========100%, Time:  03:02:41.59, cum_loss = 523442.575\n",
      "5 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 523457.53125\n",
      "Current score on wordsim Task: 0.676185051156628\n",
      "0%==========100%, Time:  02:55:46.43, cum_loss = 524288.4375\n",
      "6 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 524303.50000\n",
      "Current score on wordsim Task: 0.6749061536829675\n",
      "0%==========100%, Time:  02:58:48.30, cum_loss = 525010.755\n",
      "7 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 525025.81250\n",
      "Current score on wordsim Task: 0.6749605183035327\n",
      "0%==========100%, Time:  03:02:33.69, cum_loss = 525540.625\n",
      "8 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 525555.68750\n",
      "Current score on wordsim Task: 0.6744560926496601\n",
      "0%==========100%, Time:  03:16:09.07, cum_loss = 525977.255\n",
      "9 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 525992.62500\n",
      "Current score on wordsim Task: 0.669108620088296\n",
      "0%==========100%, Time:  03:13:37.84, cum_loss = 526406.255\n",
      "10 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 526421.43750\n",
      "Current score on wordsim Task: 0.6618866383489417\n",
      "0%==========100%, Time:  03:02:09.50, cum_loss = 526817.625\n",
      "11 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 526832.68750\n",
      "Current score on wordsim Task: 0.66952426347481\n",
      "0%==========100%, Time:  03:02:32.89, cum_loss = 527191.125\n",
      "12 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 527206.06250\n",
      "Current score on wordsim Task: 0.6612676590218918\n",
      "0%==========100%, Time:  03:02:40.53, cum_loss = 527456.375\n",
      "13 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 527471.50000\n",
      "Current score on wordsim Task: 0.6600279888343876\n",
      "0%==========100%, Time:  02:56:10.99, cum_loss = 527606.9375\n",
      "14 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 527622.00000\n",
      "Current score on wordsim Task: 0.6555544091799008\n",
      "0%==========100%, Time:  02:55:51.07, cum_loss = 527677.255\n",
      "15 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 527692.31250\n",
      "Current score on wordsim Task: 0.6619050234018661\n",
      "0%==========100%, Time:  02:55:00.13, cum_loss = 527748.4375\n",
      "16 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 527763.68750\n",
      "Current score on wordsim Task: 0.661681115757567\n",
      "0%==========100%, Time:  02:55:10.22, cum_loss = 527875.875\n",
      "17 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 527891.06250\n",
      "Current score on wordsim Task: 0.6659673790640246\n",
      "0%==========100%, Time:  02:57:43.95, cum_loss = 528124.625\n",
      "18 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 528139.75000\n",
      "Current score on wordsim Task: 0.6651928876623854\n",
      "0%==========100%, Time:  02:57:42.57, cum_loss = 528456.755\n",
      "19 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 528471.87500\n",
      "Current score on wordsim Task: 0.666018600597232\n",
      "0%==========100%, Time:  02:56:30.92, cum_loss = 528885.875\n",
      "20 epoch of 20\n",
      " 244696 244697 batches, pairs 489393510, cum loss: 528901.00000\n",
      "Current score on wordsim Task: 0.6652057563122343\n",
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.001_dim100_workers2_ctxw5_neg_samples5_use_cudaTrue_iterations20.pkl to disk with ws_score: [0.6580580246735275, 0.6651399366885339, 0.6644381396072672, 0.6716186405794663, 0.676185051156628, 0.6749061536829675, 0.6749605183035327, 0.6744560926496601, 0.669108620088296, 0.6618866383489417, 0.66952426347481, 0.6612676590218918, 0.6600279888343876, 0.6555544091799008, 0.6619050234018661, 0.661681115757567, 0.6659673790640246, 0.6651928876623854, 0.666018600597232, 0.6652057563122343] \n"
     ]
    }
   ],
   "source": [
    "w2v = W2V(enwik9_wDataset, neg_samples=5, alpha=0.001,shuffle=True)\n",
    "w2v.train_with_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntm\n",
      "Saving embedding: dict_emb_OptimAdam_momentum0_nesterovFalse_step_size1_gamma1_shuffleTrue_batch_size2000_alpha0.001_dim100_workers1_ctxw5_neg_samples5_use_cudaTrue_iterations20.pkl to disk with ws_score: [0.6651699513300704, 0.6644982110172235] \n"
     ]
    }
   ],
   "source": [
    "w2v.save_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = -1*(wordsim_task(w2v.get_embedding())[0][1])\n",
    "w2v.ws_list.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "starting training\n",
      "0%======60%, Time:  94:02:32.95, cum_loss = 29342714.0\r"
     ]
    }
   ],
   "source": [
    "ws_lists=[]\n",
    "for i in range(10):\n",
    "    w2v = W2V(enwik9_wDataset, neg_samples=5, alpha=0.001,batch_size=30,iterations=2)\n",
    "    w2v.train_with_loader()\n",
    "    ws_lists.append(np.array(w2v.ws_list))\n",
    "mean_list = [np.mean(x) for x in zip(* ws_lists)]\n",
    "with open(\"mean_list_adam_10runs_enwik9\", 'wb') as output:\n",
    "    pickle.dump(mean_list, output, pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"mean_list_adam_10runs_enwik9\", 'wb') as output:\n",
    "    pickle.dump(ws_lists, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6257743894841544,\n",
       " 0.6705295692422975,\n",
       " 0.6804812375824623,\n",
       " 0.6810218187331774,\n",
       " 0.6789483524873088,\n",
       " 0.676725395051269,\n",
       " 0.6769424838360802,\n",
       " 0.6753655497078557,\n",
       " 0.6728324299609623,\n",
       " 0.672059632651494]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "with open(\"mean_list_adam_iter10\", 'rb') as output:\n",
    "        mean_list = pickle.load(output)\n",
    "mean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.538406581419507,\n",
       " 0.6228384775172953,\n",
       " 0.6515852681869152,\n",
       " 0.6629197884943001,\n",
       " 0.6663014320112144,\n",
       " 0.6652439726413314]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "with open(\"ws_lists_gensim\", 'rb') as output:\n",
    "        ws_lists = pickle.load(output)\n",
    "mean_list = [np.mean(x) for x in zip(* ws_lists)]\n",
    "mean_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e901cb14a9bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dict_emb_w2vbs7000_neg7_dim100_epochs20_ctxw10_alpha0-075decayhalf.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "with open(\"dict_emb_w2vbs7000_neg7_dim100_epochs20_ctxw10_alpha0-075decayhalf.pkl\", 'rb') as output:\n",
    "        dict_emb = pickle.load(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30364346504211426\n",
      "0.45958149433135986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5577877461910248"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "x = spatial.distance.cosine(gensim_emb['love'], gensim_emb['music'])\n",
    "y = spatial.distance.cosine(gensim_emb['anarchism'],gensim_emb['music'])\n",
    "z = spatial.distance.cosine(gensim_emb['revolution'],gensim_emb['creatine'])\n",
    "\n",
    "l = ['music','anarchism','revolution','philosophy','creatine']\n",
    "print(x)\n",
    "print(y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(5.5450, requires_grad=True),\n",
       " tensor(5.5439, requires_grad=True),\n",
       " tensor(5.5429, requires_grad=True),\n",
       " tensor(5.5426, requires_grad=True),\n",
       " tensor(5.5412, requires_grad=True),\n",
       " tensor(5.5425, requires_grad=True),\n",
       " tensor(5.5414, requires_grad=True),\n",
       " tensor(5.5402, requires_grad=True),\n",
       " tensor(5.5409, requires_grad=True),\n",
       " tensor(5.5391, requires_grad=True),\n",
       " tensor(5.5376, requires_grad=True),\n",
       " tensor(5.5360, requires_grad=True),\n",
       " tensor(5.5368, requires_grad=True),\n",
       " tensor(5.5365, requires_grad=True),\n",
       " tensor(5.5357, requires_grad=True),\n",
       " tensor(5.5351, requires_grad=True),\n",
       " tensor(5.5354, requires_grad=True),\n",
       " tensor(5.5333, requires_grad=True),\n",
       " tensor(5.5336, requires_grad=True)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_emb.pop('loss_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_emb = dict()\n",
    "for sentences in text8_dataset:\n",
    "    for word in sentences:\n",
    "        gensim_emb[word] = model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text8_ds1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e0e07fdce0fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallbackAny2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext8_ds1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgensim_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text8_ds1' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "vocab = set(text8_ds1)\n",
    "gensim_emb = dict()\n",
    "\n",
    "    \n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.cum_loss = 0\n",
    "        self.loss_list = []\n",
    "        self.ws_list = []\n",
    "        self.prev_score = -1\n",
    "        self.no_improvement =0\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        for word in vocab:\n",
    "            gensim_emb[word] = model.wv[word]\n",
    "            \n",
    "        score = -1*wordsim_task(gensim_emb)[0][1]\n",
    "        self.ws_list.append(score)\n",
    "        \n",
    "        if(score - self.prev_score < 0.0009):\n",
    "            self.no_improvement +=1\n",
    "            \n",
    "        print(\"Epoch #{} end: cum_loss={}, ws_score={}\".format(self.epoch,self.cum_loss,score))\n",
    "        \n",
    "        \n",
    "        if(self.no_improvement == 2):\n",
    "            print(\"No improvement in word similarity early stoppage\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.epoch += 1\n",
    "        self.prev_score = score\n",
    "    \n",
    "    def on_batch_end(self, model):\n",
    "        \"\"\"Method called at the end of each batch.\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : :class:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel`\n",
    "            Current model.\n",
    "        \"\"\"\n",
    "        self.cum_loss += model.get_latest_training_loss()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(word):\n",
    "    for x in dict_emb.keys():\n",
    "        yield(x, spatial.distance.cosine(dict_emb[word],dict_emb[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0927695588b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_dict_emb_gensim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-0927695588b9>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_dict_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_dict_emb_gensim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgensim_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2449\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2450\u001b[0m                 \u001b[0msqnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2451\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2452\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2453\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "n_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "n_dict_emb_gensim = {(word): (x / np.linalg.norm(x)) for (word, x) in (gensim_emb.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALOGY TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()\n",
    "qeustions_vocab = set()\n",
    "for i,x in enumerate(questions): \n",
    "    questions[i] = x.rstrip(\"\\n\").split()\n",
    "    if x[0]==':':\n",
    "        del questions[i]\n",
    "    else: \n",
    "        for word in x:\n",
    "            questions_vocab.add(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_object, chunk_size=1024):\n",
    "    \"\"\"Lazy function (generator) to read a file piece by piece.\n",
    "    Default chunk size: 1k.\"\"\"\n",
    "    while True:\n",
    "        data = file_object.read(chunk_size)\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./data/enwik9\")\n",
    "enwik9 = read_in_chunks(file)\n",
    "l = []\n",
    "for x in enwik9:\n",
    "    l.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'social relations based upon voluntary association of autonomous individuals mutual aid and self governance while anarchism is most easily defined by what it is against anarchists also offer positive visions of what they believe to be a truly free society however ideas about how an anarchist society might work vary considerably especially with respect to economics there is also disagreement about how a free society might be brought about origins and predecessors kropotkin and others argue that before recorded history human society was organized on anarchist principles most anthropologists follow kropotkin and engels in believing that hunter gatherer bands were egalitarian and lacked division of labour accumulated wealth or decreed law and had equal access to resources william godwin anarchists including the the anarchy organisation and rothbard find anarchist attitudes in taoism from ancient china kropotkin found similar ideas in stoic zeno of citium according to kropotkin zeno repudiated the omnipotence of th'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"./data/questions-words.txt\")\n",
    "questions = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EpochLogger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8804cada497e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TODO: logging, save loss, batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mepoch_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpochLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EpochLogger' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO: logging, save loss, batch_size\n",
    "epoch_logger = EpochLogger()\n",
    "model = Word2Vec(l, size=100,window=5,negative=10, alpha=0.01, min_count=5, workers=4,sg=1, callbacks=[epoch_logger],compute_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_task(questions,dict_emb):\n",
    "    score = []\n",
    "    if all(word in dict_emb for word in questions):\n",
    "        y = dict_emb[questions[0]] -  dict_emb[questions[1]] +  dict_emb[questions[2]]\n",
    "        x = get_closest_with_score(dict_emb,y)\n",
    "        if x == questions[3]:\n",
    "            score.append(1)\n",
    "        else: \n",
    "            score.append(0)\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# TODO: calculate closest only for a given set of words to get faster computation for analogy task\n",
    "def calculate_sim(dict_emb): \n",
    "    # Create dictionnary with id for every word, this is needed because sometimes we only have access to the dict_emb\n",
    "    # and not the whole model \n",
    "    idx2word = {idx: w for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    word2idx = {w: idx for (idx, w) in enumerate(dict_emb.keys())}\n",
    "    \n",
    "    emb_size = len(next(iter(dict_emb.values())))\n",
    "    \n",
    "    # Create an embedding dictionnary with normalized vectors\n",
    "    normalized_dict_emb = {(word): (x / np.linalg.norm(x)) for (word, x) in (dict_emb.items())}\n",
    "    \n",
    "    # Create an vocab_size*emb_size Matrix that holds the normalized embeding of each word in it's row called matrix_row\n",
    "    # Create an emb_size*vocab_size Matrix that holds the normalized embeding of each word in it's colomn  matrix_colomn\n",
    "    for i in range(0,len(dict_emb.keys())):\n",
    "        y = normalized_dict_emb[idx2word[i]]\n",
    "        if i ==0:\n",
    "            matrix_colomn = torch.tensor(y).view(emb_size,1)\n",
    "            matrix_row = torch.tensor(y)\n",
    "        else:\n",
    "            matrix_colomn = torch.cat([matrix_colomn,torch.tensor(y).view(emb_size,1)],1)\n",
    "            matrix_row = torch.cat([matrix_row,torch.tensor(y)])\n",
    "    \n",
    "    matrix_row = matrix_row.view(-1,emb_size)\n",
    "    \n",
    "    matrix_row = matrix_row.to(device)\n",
    "    matrix_colomn = matrix_colomn.to(device)\n",
    "    \n",
    "    return 1-(torch.matmul(matrix_row,matrix_colomn)),word2idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_closest(score_dict, word):\n",
    "    closest = ()\n",
    "    distance = 3\n",
    "    for (x,y),score in score_dict.items():\n",
    "        #print(x,y,score)\n",
    "        if((x != y) and ((x==word)or(y==word))):\n",
    "            if (distance > score):\n",
    "                closest = (x,y)\n",
    "                distance = score\n",
    "    return closest\n",
    "\n",
    "def get_closest_with_score(dict_emb,y):\n",
    "    distance = 100\n",
    "    for x,emb in dict_emb.items():\n",
    "        if(spatial.distance.cosine(dict_emb[x], dict_emb[y])<distance):\n",
    "            closest = x\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "list_of_files = []\n",
    "for file in os.listdir(\"/home/c3dric/model/todo\"):\n",
    "        list_of_files.append(file)\n",
    "list_of_files.remove('.ipynb_checkpoints')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_files.remove('shuffle_false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_emb = []\n",
    "for i,file in enumerate(list_of_files):\n",
    "    with open(\"/home/c3dric/model/todo/\" + file, 'rb') as output:\n",
    "        dict_emb = pickle.load(output)\n",
    "        file_emb.append((file, [float(x) for x in dict_emb['ws_list']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the saved data into a csv file\n",
    "import re\n",
    "import csv\n",
    "with_0 = r'(\\w*)alpha(\\d.\\d*)_\\w*'\n",
    "without_0 = r'(\\w*)alpha(\\d)_\\w*'\n",
    "sgd_reg = r'(\\w*)momentum0_.*'\n",
    "models_sgd = []\n",
    "models_adam = []\n",
    "models_adagrad = []\n",
    "models_mom = []\n",
    "models_nag = []\n",
    "for model in file_emb:\n",
    "    filename = model[0] \n",
    "    if \"OptimAdagrad\" in filename: \n",
    "        models_adagrad.append(model)\n",
    "    if \"OptimAdam\" in filename: \n",
    "        models_adam.append(model)\n",
    "    if \"OptimSGD\" in filename:\n",
    "        if(re.search(sgd_reg,filename)):\n",
    "            models_sgd.append(model)\n",
    "        elif(\"nesterovFalse\" in filename):\n",
    "            models_mom.append(model)\n",
    "        else:\n",
    "            models_nag.append(model)\n",
    "        \n",
    "assert(len(file_emb) == len(models_sgd + models_adam + models_adagrad + models_mom + models_nag))\n",
    "\n",
    "def create_csv(models,csv_file_name):\n",
    "    lr = []\n",
    "    lr_scores = []\n",
    "    epochs = [[] for x in range(20)]\n",
    "    for model in models:\n",
    "        filename = model[0]\n",
    "        if(re.search (without_0,filename)):\n",
    "            alpha =  int(re.search(without_0,filename).group(2))\n",
    "            lr_scores.append((alpha,model[1]))\n",
    "        if(re.search (with_0,filename)):\n",
    "            alpha =  float(re.search(with_0,filename).group(2))\n",
    "            lr_scores.append((alpha,model[1]))\n",
    "    lr_scores = sorted(lr_scores)\n",
    "    lr = [x[0] for x in lr_scores]\n",
    "    scores = [x[1] for  x in lr_scores]\n",
    "    \n",
    "    for x in lr_scores: \n",
    "        ws_scores = x[1]\n",
    "        for i,score in enumerate(ws_scores):\n",
    "            epochs[i].append(score)\n",
    "        for j in range(i+1,20):\n",
    "            epochs[j].append(\"\")\n",
    "            \n",
    "    \n",
    "    output = [lr] + epochs \n",
    "    \n",
    "    with open(csv_file_name, 'w') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerows(output)\n",
    "        csvFile.close()\n",
    "\n",
    "\n",
    "create_csv(models_adam,\"adam.csv\")\n",
    "create_csv(models_adagrad,\"adagrad.csv\")\n",
    "create_csv(models_sgd,\"sgd.csv\")\n",
    "create_csv(models_mom,\"mom.csv\")\n",
    "create_csv(models_nag,\"nag.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Taken from https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word2vec.ipynb##\n",
    "import logging\n",
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def reduce_dimensions(model,vocab, vocab_plot, plot_in_notebook = True):\n",
    "\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    vectors = []        # positions in vector space\n",
    "    labels = []         # keep track of words to label our data again later\n",
    "    for word in vocab:\n",
    "        vectors.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    \n",
    "    # convert both lists into numpy vectors for reduction\n",
    "    vectors = np.asarray(vectors)\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # reduce using t-SNE\n",
    "    vectors = np.asarray(vectors)\n",
    "    logging.info('starting tSNE dimensionality reduction. This may take some time.')\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    \n",
    "    x_vals = [v[0] for i,v in enumerate(vectors) if labels[i] in vocab_plot]\n",
    "    y_vals = [v[1] for i,v in enumerate(vectors) if labels[i] in vocab_plot]\n",
    "        \n",
    "    # Create a trace\n",
    "    trace = go.Scatter(\n",
    "        x=x_vals,\n",
    "        y=y_vals,\n",
    "        mode='text',\n",
    "        text=labels\n",
    "        )\n",
    "    \n",
    "    data = [trace]\n",
    "    \n",
    "    logging.info('All done. Plotting.')\n",
    "    \n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['popcorn', 'fruit', 'eat', 'grocery', 'potato', 'seafood', 'butter', 'cabbage', 'drink', 'wine', 'brandy', 'vodka', 'oil', 'grocery', 'cucumber', 'wine']\n",
      "['card', 'luxury', 'gem', 'investor', 'rading', 'earning', 'insuracnce,', 'dividend', 'profit', 'wealth', 'market', 'stock', 'payment', 'money']\n"
     ]
    }
   ],
   "source": [
    "money_vocab = 'card luxury gem investor rading earning insuracnce, dividend profit wealth market stock payment money'.split(\" \")\n",
    "foods_vocab = \"popcorn fruit eat grocery potato seafood butter cabbage drink wine brandy vodka oil grocery cucumber wine \".split()\n",
    "print(foods_vocab)\n",
    "print(money_vocab)\n",
    "dict_emb.pop(\"time\")\n",
    "dict_emb.pop(\"loss_list\")\n",
    "    \n",
    "reduce_dimensions(dict_emb, dict_emb.keys(),money_vocab + foods_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(20,125,190)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "sgd",
         "type": "scatter",
         "uid": "9fae3d89-8bab-4d46-bf10-01979cd719e8",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8,
          9,
          10,
          11,
          12
         ],
         "y": [
          20,
          20,
          20,
          20,
          20,
          16,
          11,
          20,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(245,145,30)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adam",
         "type": "scatter",
         "uid": "cdcca733-aace-4bd5-b374-739e22d8a2b4",
         "x": [
          1,
          3,
          5,
          8,
          12
         ],
         "y": [
          20,
          14,
          5,
          3,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(25,160,75)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adagrad",
         "type": "scatter",
         "uid": "443297e2-65a6-47dc-b2d3-227375149c16",
         "x": [
          4,
          5,
          12,
          13,
          14,
          15,
          16,
          17,
          23
         ],
         "y": [
          20,
          20,
          16,
          13,
          4,
          6,
          11,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(200,30,135)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "momentum",
         "type": "scatter",
         "uid": "b5cd40d4-f502-47f0-84df-752074a06502",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8
         ],
         "y": [
          20,
          20,
          13,
          11,
          9,
          20,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(0,0,0)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "nag",
         "type": "scatter",
         "uid": "a043cd0e-3182-4585-ab1a-7257feaadbbb",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8
         ],
         "y": [
          20,
          15,
          12,
          12,
          20,
          20
         ]
        }
       ],
       "layout": {
        "height": 500,
        "title": {
         "text": "Time to train vs. learning rate, by optimizer"
        },
        "width": 800,
        "xaxis": {
         "ticktext": [
          5e-05,
          0.0001,
          0.00025,
          0.0005,
          0.00075,
          0.001,
          0.002,
          0.0025,
          0.005,
          0.0075,
          0.01,
          0.025,
          0.05,
          0.075,
          0.1,
          0.25,
          0.5,
          1,
          1.5,
          2,
          2.5,
          3,
          4,
          5,
          7.5,
          10,
          15,
          17.5,
          20,
          22.5,
          25,
          30,
          32.5,
          35,
          40,
          45,
          50
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "title": {
          "text": "Learning rate"
         }
        },
        "yaxis": {
         "autorange": true,
         "tickvals": [
          1,
          2,
          3,
          4,
          5,
          9,
          11,
          12,
          20
         ],
         "title": {
          "text": "Training time in number of Epochs"
         },
         "type": "linear"
        }
       }
      },
      "text/html": [
       "<div id=\"a61591d3-14ab-4feb-b54b-3a43a2eac4f0\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a61591d3-14ab-4feb-b54b-3a43a2eac4f0\")) {\n",
       "    Plotly.newPlot(\"a61591d3-14ab-4feb-b54b-3a43a2eac4f0\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12], \"y\": [20, 20, 20, 20, 20, 16, 11, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"cf93e276-7ea4-49b2-8073-806c2e526f5b\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam\", \"x\": [1, 3, 5, 8, 12], \"y\": [20, 14, 5, 3, 20], \"type\": \"scatter\", \"uid\": \"14d124e7-53f0-40f6-a323-226357dc8b2d\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 23], \"y\": [20, 20, 16, 13, 4, 6, 11, 20, 20], \"type\": \"scatter\", \"uid\": \"52984550-727b-40b3-a8a0-98c9c20b9ca5\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8], \"y\": [20, 20, 13, 11, 9, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"6e998eda-a4bf-4cc7-9f04-9141b89bc371\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag\", \"x\": [2, 3, 4, 5, 7, 8], \"y\": [20, 15, 12, 12, 20, 20], \"type\": \"scatter\", \"uid\": \"7ed5f19b-9ca8-4a26-a9d6-4bc2d05243c2\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"tickvals\": [1, 2, 3, 4, 5, 9, 11, 12, 20], \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"a61591d3-14ab-4feb-b54b-3a43a2eac4f0\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"a61591d3-14ab-4feb-b54b-3a43a2eac4f0\")) {\n",
       "    Plotly.newPlot(\"a61591d3-14ab-4feb-b54b-3a43a2eac4f0\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12], \"y\": [20, 20, 20, 20, 20, 16, 11, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"cf93e276-7ea4-49b2-8073-806c2e526f5b\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam\", \"x\": [1, 3, 5, 8, 12], \"y\": [20, 14, 5, 3, 20], \"type\": \"scatter\", \"uid\": \"14d124e7-53f0-40f6-a323-226357dc8b2d\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 23], \"y\": [20, 20, 16, 13, 4, 6, 11, 20, 20], \"type\": \"scatter\", \"uid\": \"52984550-727b-40b3-a8a0-98c9c20b9ca5\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8], \"y\": [20, 20, 13, 11, 9, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"6e998eda-a4bf-4cc7-9f04-9141b89bc371\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag\", \"x\": [2, 3, 4, 5, 7, 8], \"y\": [20, 15, 12, 12, 20, 20], \"type\": \"scatter\", \"uid\": \"7ed5f19b-9ca8-4a26-a9d6-4bc2d05243c2\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"tickvals\": [1, 2, 3, 4, 5, 9, 11, 12, 20], \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "learning_rates = [5e-05, 0.0001,0.00025,0.0005,.00075,.001,0.002,.0025,0.005,0.0075,.01,.025,.05,0.075,0.1,0.25,0.5] + [1,1.5,2,2.5,3,4,5,7.5,10,15,17.5,20,22.5,25,30,32.5,35,40,45,50]\n",
    "lr_dict = {x: i for i,x in enumerate(learning_rates)}\n",
    "# SGD\n",
    "lr_sgd      = [0.00025,0.0005,0.00075,0.001,0.0025,0.005,0.0075,0.01,0.025,0.05]\n",
    "tr_time_sgd = [20,20,20,20,20,16,11,20,20,20]\n",
    "lr_sgd_dict = [lr_dict[x] for x in lr_sgd]\n",
    "assert(len(lr_sgd)== len(tr_time_sgd))\n",
    "# ADAM\n",
    "lr_adam = [0.0001,0.0005,0.001,0.005,0.05]\n",
    "lr_adam_dict = [lr_dict[x] for x in lr_adam]\n",
    "tr_time_adam = [20,14,5,3,20]\n",
    "# adagrad\n",
    "lr_adagrad = [0.00075,0.001,0.05,0.075,0.1,0.25,0.5,1,5]\n",
    "lr_adagrad_dict = [lr_dict[x] for x in lr_adagrad]\n",
    "tr_time_adagrad = [20,20,16,13,4,6,11,20,20]\n",
    "# momentum\n",
    "lr_mom = [0.0001,0.00025,0.0005,0.00075,0.001,0.002,0.0025,0.005] \n",
    "lr_mom_dict = [lr_dict[x] for x in lr_mom]\n",
    "tr_time_mom = [20,20,13,11,9,20,20,20]\n",
    "# NAG\n",
    "lr_nag = [0.00025,0.0005,0.00075,0.001,0.0025,0.005]\n",
    "lr_nag_dict = [lr_dict[x] for x in lr_nag]\n",
    "tr_time_nag = [20,15,12,12,20,20]\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=lr_sgd_dict,\n",
    "    y=tr_time_sgd,\n",
    "    mode='lines+markers',\n",
    "    name=\"sgd\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(20,125,190)'\n",
    "    )\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x= lr_adam_dict,\n",
    "    y=tr_time_adam,\n",
    "    mode='lines+markers',\n",
    "    name=\"adam\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(245,145,30)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x= lr_adagrad_dict,\n",
    "    y=tr_time_adagrad,\n",
    "    mode='lines+markers',\n",
    "    name=\"adagrad\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(25,160,75)'\n",
    "    )\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x= lr_mom_dict,\n",
    "    y=tr_time_mom,\n",
    "    mode='lines+markers',\n",
    "    name=\"momentum\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(200,30,135)'\n",
    "    )\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x= lr_nag_dict,\n",
    "    y=tr_time_nag,\n",
    "    mode='lines+markers',\n",
    "    name=\"nag\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(0,0,0)'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4,trace5]\n",
    "layout = dict(title = 'Time to train vs. learning rate, by optimizer',\n",
    "                 width = 800,\n",
    "    height = 500,\n",
    "    xaxis = dict(\n",
    "        tickvals = list(lr_dict.values()),\n",
    "        ticktext = list(lr_dict.keys()),\n",
    "      title = \"Learning rate\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        title = 'Training time in number of Epochs',\n",
    "        tickvals= [1,2,3,4,5,9,11,12,20],\n",
    "        autorange=True\n",
    "    )\n",
    "              )\n",
    "\n",
    "print(layout[\"xaxis\"][\"tickvals\"])\n",
    "fig = dict(data=data, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "iplot(fig, filename='word-embedding-plot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(20,125,190)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "sgd_shuffle",
         "type": "scatter",
         "uid": "bfc8e278-cf89-47aa-8d3b-0b451a5988cf",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14
         ],
         "y": [
          20,
          20,
          20,
          20,
          20,
          19,
          11,
          11,
          11,
          7,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(245,145,30)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adam_shuffle",
         "type": "scatter",
         "uid": "949ddd68-8805-45e1-82d7-ba4394681be1",
         "x": [
          3,
          5,
          8,
          12,
          14,
          16,
          17,
          18
         ],
         "y": [
          20,
          2,
          2,
          7,
          8,
          15,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(25,160,75)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adagrad_shuffle",
         "type": "scatter",
         "uid": "0f4cf6fe-387d-4978-a46f-c2d65227848d",
         "x": [
          4,
          5,
          12,
          13,
          14,
          15,
          16,
          17,
          19,
          23
         ],
         "y": [
          20,
          20,
          4,
          3,
          3,
          8,
          20,
          20,
          20,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(200,30,135)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "momentum_suffle",
         "type": "scatter",
         "uid": "96b90d0a-107f-43db-a693-8102b03566b2",
         "x": [
          3,
          4,
          6,
          7,
          12
         ],
         "y": [
          17,
          12,
          8,
          9,
          20
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(0,0,0)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "nag_shuffle",
         "type": "scatter",
         "uid": "3e216455-26f9-4268-988f-87274816c68f",
         "x": [
          2,
          3,
          4,
          5,
          7,
          8,
          9,
          12
         ],
         "y": [
          20,
          19,
          9,
          9,
          7,
          3,
          20,
          20
         ]
        }
       ],
       "layout": {
        "height": 500,
        "title": {
         "text": "Time to train vs. learning rate, by optimizer"
        },
        "width": 800,
        "xaxis": {
         "ticktext": [
          5e-05,
          0.0001,
          0.00025,
          0.0005,
          0.00075,
          0.001,
          0.002,
          0.0025,
          0.005,
          0.0075,
          0.01,
          0.025,
          0.05,
          0.075,
          0.1,
          0.25,
          0.5,
          1,
          1.5,
          2,
          2.5,
          3,
          4,
          5,
          7.5,
          10,
          15,
          17.5,
          20,
          22.5,
          25,
          30,
          32.5,
          35,
          40,
          45,
          50
         ],
         "tickvals": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "title": {
          "text": "Learning rate"
         }
        },
        "yaxis": {
         "autorange": true,
         "tickvals": [
          1,
          2,
          3,
          7,
          8,
          15,
          20
         ],
         "title": {
          "text": "Training time in number of Epochs"
         },
         "type": "linear"
        }
       }
      },
      "text/html": [
       "<div id=\"5701084d-2a85-463c-8c20-da4444421b78\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"5701084d-2a85-463c-8c20-da4444421b78\")) {\n",
       "    Plotly.newPlot(\"5701084d-2a85-463c-8c20-da4444421b78\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [20, 20, 20, 20, 20, 19, 11, 11, 11, 7, 20, 20], \"type\": \"scatter\", \"uid\": \"e8e4cc85-be3c-4010-a3d7-79d8a28308e8\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [3, 5, 8, 12, 14, 16, 17, 18], \"y\": [20, 2, 2, 7, 8, 15, 20, 20], \"type\": \"scatter\", \"uid\": \"de151cc9-4f9f-403f-922c-3aee35fc2ee7\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 19, 23], \"y\": [20, 20, 4, 3, 3, 8, 20, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"db04b724-b21a-4622-8947-2263a735577f\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum_suffle\", \"x\": [3, 4, 6, 7, 12], \"y\": [17, 12, 8, 9, 20], \"type\": \"scatter\", \"uid\": \"731c3555-9d5f-43c1-bddf-0265564d2eba\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 9, 12], \"y\": [20, 19, 9, 9, 7, 3, 20, 20], \"type\": \"scatter\", \"uid\": \"fb5d9975-619f-4125-a456-40de22d944e7\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"tickvals\": [1, 2, 3, 7, 8, 15, 20], \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5701084d-2a85-463c-8c20-da4444421b78\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"5701084d-2a85-463c-8c20-da4444421b78\")) {\n",
       "    Plotly.newPlot(\"5701084d-2a85-463c-8c20-da4444421b78\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"sgd_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14], \"y\": [20, 20, 20, 20, 20, 19, 11, 11, 11, 7, 20, 20], \"type\": \"scatter\", \"uid\": \"e8e4cc85-be3c-4010-a3d7-79d8a28308e8\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [3, 5, 8, 12, 14, 16, 17, 18], \"y\": [20, 2, 2, 7, 8, 15, 20, 20], \"type\": \"scatter\", \"uid\": \"de151cc9-4f9f-403f-922c-3aee35fc2ee7\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(25,160,75)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adagrad_shuffle\", \"x\": [4, 5, 12, 13, 14, 15, 16, 17, 19, 23], \"y\": [20, 20, 4, 3, 3, 8, 20, 20, 20, 20], \"type\": \"scatter\", \"uid\": \"db04b724-b21a-4622-8947-2263a735577f\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(200,30,135)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"momentum_suffle\", \"x\": [3, 4, 6, 7, 12], \"y\": [17, 12, 8, 9, 20], \"type\": \"scatter\", \"uid\": \"731c3555-9d5f-43c1-bddf-0265564d2eba\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(0,0,0)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"nag_shuffle\", \"x\": [2, 3, 4, 5, 7, 8, 9, 12], \"y\": [20, 19, 9, 9, 7, 3, 20, 20], \"type\": \"scatter\", \"uid\": \"fb5d9975-619f-4125-a456-40de22d944e7\"}], {\"height\": 500, \"title\": {\"text\": \"Time to train vs. learning rate, by optimizer\"}, \"width\": 800, \"xaxis\": {\"ticktext\": [5e-05, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5, 7.5, 10, 15, 17.5, 20, 22.5, 25, 30, 32.5, 35, 40, 45, 50], \"tickvals\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36], \"title\": {\"text\": \"Learning rate\"}}, \"yaxis\": {\"autorange\": true, \"tickvals\": [1, 2, 3, 7, 8, 15, 20], \"title\": {\"text\": \"Training time in number of Epochs\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#learning_rates = [0.00025,0.0005,.00075,.001,0.1,0.25,0.5] + [2.5,5,7.5,10,15,20,22.5]\n",
    "#lr_dict = {x: i for i,x in enumerate(learning_rates)}\n",
    "# sgd_shuffle\n",
    "lr_sgd_shuffle    = [0.00025,0.0005,0.00075,0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1]\n",
    "tr_time_sgd_shuffle = [20,20,20,20,20,19,11,11,11,7,20,20]\n",
    "lr_sgd_shuffle_dict = [lr_dict[x] for x in lr_sgd_shuffle]\n",
    "assert len(lr_sgd_shuffle) == len(tr_time_sgd_shuffle)\n",
    "\n",
    "# adam_shuffle\n",
    "lr_adam_shuffle = [0.0001,0.0005,0.001,0.005,0.05,0.1]\n",
    "lr_adam_shuffle_dict = [lr_dict[x] for x in lr_adam_shuffle]\n",
    "tr_time_adam_shuffle = [20,4,2,157,8,15,20,20]\n",
    "assert len(lr_adam_shuffle) == len(tr_time_adam_shuffle), (str(len(lr_adam_shuffle))+ \" \"+ str(len(tr_time_adam_shuffle)))\n",
    "\n",
    "# adagrad_shuffle\n",
    "lr_adagrad_shuffle = [0.00075,0.001,0.05,0.075,0.1,0.25,0.5,1,2,5]\n",
    "lr_adagrad_shuffle_dict = [lr_dict[x] for x in lr_adagrad_shuffle]\n",
    "tr_time_adagrad_shuffle = [20,20,4,3,3,8,20,20,20,20]\n",
    "assert len(lr_adagrad_shuffle) == len(tr_time_adagrad_shuffle), (str(len(lr_adagrad_shuffle))+ \" \"+ str(len(tr_time_adagrad_shuffle)))\n",
    "\n",
    "\n",
    "# mom_suffleentum\n",
    "lr_mom_suffle = [0.0005,0.00075,0.002,0.0025,0.05] \n",
    "lr_mom_suffle_dict = [lr_dict[x] for x in lr_mom_suffle]\n",
    "tr_time_mom_suffle = [17,12,8,9,20]\n",
    "assert len(lr_mom_suffle) == len(tr_time_mom_suffle), (str(len(lr_mom_suffle))+ \" \"+ str(len(tr_time_mom_suffle)))\n",
    "\n",
    "\n",
    "# nag_shuffle\n",
    "lr_nag_shuffle = [0.00025,0.0005,0.00075,0.001,0.0025,0.005,0.0075,0.05]\n",
    "lr_nag_shuffle_dict = [lr_dict[x] for x in lr_nag_shuffle]\n",
    "tr_time_nag_shuffle = [20,19,9,9,7,3,20,20]\n",
    "assert len(lr_nag_shuffle) == len(tr_time_nag_shuffle),(str(len(lr_nag_shuffle))+ \" \"+ str(len(tr_time_nag_shuffle)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trace1a = go.Scatter(\n",
    "    x=lr_sgd_shuffle_dict,\n",
    "    y=tr_time_sgd_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"sgd_shuffle\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(20,125,190)'\n",
    "    )\n",
    ")\n",
    "trace2a = go.Scatter(\n",
    "    x= lr_adam_shuffle_dict,\n",
    "    y=tr_time_adam_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"adam_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(245,145,30)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace3a = go.Scatter(\n",
    "    x= lr_adagrad_shuffle_dict,\n",
    "    y=tr_time_adagrad_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"adagrad_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(25,160,75)'\n",
    "    )\n",
    ")\n",
    "trace4a = go.Scatter(\n",
    "    x= lr_mom_suffle_dict,\n",
    "    y=tr_time_mom_suffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"momentum_suffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(200,30,135)'\n",
    "    )\n",
    ")\n",
    "trace5a = go.Scatter(\n",
    "    x= lr_nag_shuffle_dict,\n",
    "    y=tr_time_nag_shuffle,\n",
    "    mode='lines+markers',\n",
    "    name=\"nag_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(0,0,0)'\n",
    "    )\n",
    ")\n",
    "data_a = [trace1a,trace2a,trace3a,trace4a,trace5a] \n",
    "data = data + data_a\n",
    "layout = dict(title = 'Time to train vs. learning rate, by optimizer',\n",
    "                 width = 800,\n",
    "    height = 500,\n",
    "    xaxis = dict(\n",
    "        tickvals = list(lr_dict.values()),\n",
    "        ticktext = list(lr_dict.keys()),\n",
    "      title = \"Learning rate\"\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        title = 'Training time in number of Epochs',\n",
    "        tickvals=[1,2,3,7,8,15,20],\n",
    "        autorange=True\n",
    "    )\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "fig1 = dict(data=data_a, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "#iplot(fig, filename='word-embedding-plot')\n",
    "iplot(fig1, filename='word-embedding-plot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(139,0,0)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "gensim",
         "type": "scatter",
         "uid": "5c8b7ec2-d4af-4f24-ad23-fbafe04424d2",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.47,
          0.59,
          0.63,
          0.637,
          0.649,
          0.64991,
          0.653,
          0.664,
          0.6655,
          0.6615,
          0.6629,
          0.663
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(20,125,190)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "SGD_shuffle",
         "type": "scatter",
         "uid": "d373b57d-2b1b-4a60-8122-c9e7d2fb38c0",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.3979027932,
          0.5345590809,
          0.5957749206,
          0.6150409906,
          0.6343477449,
          0.6408864911,
          0.6569273384,
          0.6638613219,
          0.664644281,
          0.6684268463,
          0.6676506492
         ]
        },
        {
         "hoverinfo": "name",
         "line": {
          "color": "rgb(20,125,190)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "gensim",
         "type": "scatter",
         "uid": "c85364dd-6a81-4a03-94d0-e6d011ab9a02",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.2656567075,
          0.409453666,
          0.4961671944,
          0.5470711378,
          0.58201276,
          0.608320445,
          0.6224159501,
          0.6369093127,
          0.6457120334,
          0.653658488,
          0.6585042924,
          0.6608316399
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(245,145,30)",
          "dash": "dot",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adam_shuffle",
         "type": "scatter",
         "uid": "3176425e-2746-428e-a14f-7993511e3afd",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.6209724591,
          0.6648207026,
          0.6608316399,
          0.6653500479,
          0.6667224712
         ]
        },
        {
         "hoverinfo": "text+name",
         "line": {
          "color": "rgb(245,145,30)",
          "shape": "linear"
         },
         "mode": "lines+markers",
         "name": "adam_shuffle",
         "type": "scatter",
         "uid": "f14efa63-6541-490e-bcb1-34c138500202",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19
         ],
         "y": [
          0,
          0.5274663249,
          0.6317800941,
          0.6661224712,
          0.6663352711,
          0.6692290085,
          0.6709985856,
          0.6687059277,
          0.6676404941
         ]
        }
       ],
       "layout": {
        "height": 500,
        "title": {
         "text": "Convergence time comparison"
        },
        "width": 800,
        "xaxis": {
         "title": {
          "text": "Training time in number of Epochs"
         }
        },
        "yaxis": {
         "autorange": true,
         "title": {
          "text": "Word similarity"
         },
         "type": "linear"
        }
       }
      },
      "text/html": [
       "<div id=\"8a35eeec-8a18-4e42-8684-4690508dcf04\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"8a35eeec-8a18-4e42-8684-4690508dcf04\")) {\n",
       "    Plotly.newPlot(\"8a35eeec-8a18-4e42-8684-4690508dcf04\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(139,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.47, 0.59, 0.63, 0.637, 0.649, 0.64991, 0.653, 0.664, 0.6655, 0.6615, 0.6629, 0.663], \"type\": \"scatter\", \"uid\": \"be9e8749-976d-4609-b607-ff7766bfcb9d\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"SGD_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.3979027932, 0.5345590809, 0.5957749206, 0.6150409906, 0.6343477449, 0.6408864911, 0.6569273384, 0.6638613219, 0.664644281, 0.6684268463, 0.6676506492], \"type\": \"scatter\", \"uid\": \"8ac49cd4-cd26-49f7-b337-3901cabe544d\"}, {\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.2656567075, 0.409453666, 0.4961671944, 0.5470711378, 0.58201276, 0.608320445, 0.6224159501, 0.6369093127, 0.6457120334, 0.653658488, 0.6585042924, 0.6608316399], \"type\": \"scatter\", \"uid\": \"e066aafc-c475-48a1-88bd-30ccb8fc7b30\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.6209724591, 0.6648207026, 0.6608316399, 0.6653500479, 0.6667224712], \"type\": \"scatter\", \"uid\": \"e6ed0982-e110-43f0-aa8a-8db539a2bb01\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.5274663249, 0.6317800941, 0.6661224712, 0.6663352711, 0.6692290085, 0.6709985856, 0.6687059277, 0.6676404941], \"type\": \"scatter\", \"uid\": \"74fa9a02-3fd8-4475-ad4f-7cc792ed7e8e\"}], {\"height\": 500, \"title\": {\"text\": \"Convergence time comparison\"}, \"width\": 800, \"xaxis\": {\"title\": {\"text\": \"Training time in number of Epochs\"}}, \"yaxis\": {\"autorange\": true, \"title\": {\"text\": \"Word similarity\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8a35eeec-8a18-4e42-8684-4690508dcf04\" style=\"height: 500px; width: 800px;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"8a35eeec-8a18-4e42-8684-4690508dcf04\")) {\n",
       "    Plotly.newPlot(\"8a35eeec-8a18-4e42-8684-4690508dcf04\", [{\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(139,0,0)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.47, 0.59, 0.63, 0.637, 0.649, 0.64991, 0.653, 0.664, 0.6655, 0.6615, 0.6629, 0.663], \"type\": \"scatter\", \"uid\": \"be9e8749-976d-4609-b607-ff7766bfcb9d\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"SGD_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.3979027932, 0.5345590809, 0.5957749206, 0.6150409906, 0.6343477449, 0.6408864911, 0.6569273384, 0.6638613219, 0.664644281, 0.6684268463, 0.6676506492], \"type\": \"scatter\", \"uid\": \"8ac49cd4-cd26-49f7-b337-3901cabe544d\"}, {\"hoverinfo\": \"name\", \"line\": {\"color\": \"rgb(20,125,190)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"gensim\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.2656567075, 0.409453666, 0.4961671944, 0.5470711378, 0.58201276, 0.608320445, 0.6224159501, 0.6369093127, 0.6457120334, 0.653658488, 0.6585042924, 0.6608316399], \"type\": \"scatter\", \"uid\": \"e066aafc-c475-48a1-88bd-30ccb8fc7b30\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"dash\": \"dot\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.6209724591, 0.6648207026, 0.6608316399, 0.6653500479, 0.6667224712], \"type\": \"scatter\", \"uid\": \"e6ed0982-e110-43f0-aa8a-8db539a2bb01\"}, {\"hoverinfo\": \"text+name\", \"line\": {\"color\": \"rgb(245,145,30)\", \"shape\": \"linear\"}, \"mode\": \"lines+markers\", \"name\": \"adam_shuffle\", \"x\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], \"y\": [0, 0.5274663249, 0.6317800941, 0.6661224712, 0.6663352711, 0.6692290085, 0.6709985856, 0.6687059277, 0.6676404941], \"type\": \"scatter\", \"uid\": \"74fa9a02-3fd8-4475-ad4f-7cc792ed7e8e\"}], {\"height\": 500, \"title\": {\"text\": \"Convergence time comparison\"}, \"width\": 800, \"xaxis\": {\"title\": {\"text\": \"Training time in number of Epochs\"}}, \"yaxis\": {\"autorange\": true, \"title\": {\"text\": \"Word similarity\"}, \"type\": \"linear\"}}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "gensim_conv = [0,0.47 ,0.59,0.63,0.637,0.649, 0.64991,0.653,0.664,0.6655,0.6615,0.6629,0.663] \n",
    "adam_conv = [0,0.5274663249,0.6317800941,0.6661224712,0.6663352711,0.6692290085, 0.6709985856, 0.6687059277, 0.6676404941]\n",
    "adam_shuffle_conv = [0,0.6209724591, 0.6648207026,0.6608316399,0.6653500479,0.6667224712]\n",
    "sgd_conv = [0,0.2656567075,0.409453666,0.4961671944,0.5470711378,0.58201276,0.608320445,0.6224159501,0.6369093127,0.6457120334,0.653658488,0.6585042924,0.6608316399]\n",
    "sgd_shuffle_conv = [0,0.3979027932,0.5345590809,0.5957749206,0.6150409906,0.6343477449,0.6408864911,0.6569273384,0.6638613219,0.664644281,0.6684268463,0.6676506492]\n",
    "\n",
    "epoches = list(range(20))\n",
    "trace1 = go.Scatter(\n",
    "    x=epoches,\n",
    "    y=gensim_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"gensim\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(139,0,0)'\n",
    "    )\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x= epoches,\n",
    "    y=sgd_shuffle_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"SGD_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(20,125,190)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=epoches,\n",
    "    y=sgd_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"gensim\",\n",
    "    hoverinfo='name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(20,125,190)'\n",
    "    )\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x= epoches,\n",
    "    y=adam_shuffle_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"adam_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',dash='dot',color='rgb(245,145,30)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x= epoches,\n",
    "    y=adam_conv,\n",
    "    mode='lines+markers',\n",
    "    name=\"adam_shuffle\",\n",
    "    hoverinfo='text+name',\n",
    "    line=dict(\n",
    "        shape='linear',color='rgb(245,145,30)'\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace1,trace2,trace3,trace4,trace5]\n",
    "layout = dict(title = 'Convergence time comparison',\n",
    "                 width = 800,\n",
    "    height = 500,\n",
    "    xaxis = dict(\n",
    "      title = 'Training time in number of Epochs'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        type='linear',\n",
    "        title = \"Word similarity\",\n",
    "             autorange=True\n",
    "    )\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "iplot(fig, filename='word-embedding-plot.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
